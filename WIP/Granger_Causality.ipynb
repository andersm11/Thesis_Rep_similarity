{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'all_preds'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\User\\miniconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3805\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3804\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 3805\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3806\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32mindex.pyx:167\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mindex.pyx:196\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:7081\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:7089\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'all_preds'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 20\u001b[0m\n\u001b[0;32m     17\u001b[0m history \u001b[38;5;241m=\u001b[39m run\u001b[38;5;241m.\u001b[39mhistory()\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# Extract stored predictions and true labels\u001b[39;00m\n\u001b[1;32m---> 20\u001b[0m all_preds \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(\u001b[43mhistory\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mall_preds\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m)\u001b[38;5;241m.\u001b[39mflatten()\n\u001b[0;32m     21\u001b[0m all_targets \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(history[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mall_targets\u001b[39m\u001b[38;5;124m\"\u001b[39m])\u001b[38;5;241m.\u001b[39mflatten()\n\u001b[0;32m     23\u001b[0m \u001b[38;5;66;03m# Compute confusion matrix\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\User\\miniconda3\\lib\\site-packages\\pandas\\core\\frame.py:4090\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   4088\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   4089\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[1;32m-> 4090\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4091\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[0;32m   4092\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[1;32mc:\\Users\\User\\miniconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3812\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3807\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[0;32m   3808\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[0;32m   3809\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[0;32m   3810\u001b[0m     ):\n\u001b[0;32m   3811\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[1;32m-> 3812\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m   3813\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m   3814\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   3815\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   3816\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[0;32m   3817\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'all_preds'"
     ]
    }
   ],
   "source": [
    "import wandb\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Initialize wandb API\n",
    "api = wandb.Api()\n",
    "\n",
    "# Define project and run ID\n",
    "entity = \"philinthesky\"  # Your WandB username or team name\n",
    "project_name = \"Master Thesis\"  # Change to your actual project name\n",
    "run_id = \"zd2j8s0l\" # Get this from wandb UI (not the name, but the ID)\n",
    "\n",
    "# Fetch the run\n",
    "run = api.run(f\"{entity}/{project_name}/{run_id}\")\n",
    "\n",
    "# Retrieve history as a pandas DataFrame\n",
    "history = run.history()\n",
    "\n",
    "# Extract stored predictions and true labels\n",
    "all_preds = np.array(history[\"all_preds\"]).flatten()\n",
    "all_targets = np.array(history[\"all_targets\"]).flatten()\n",
    "\n",
    "# Compute confusion matrix\n",
    "conf_matrix = confusion_matrix(all_targets[-1], all_preds[-1])\n",
    "\n",
    "print(\"Confusion Matrix:\\n\", conf_matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "idx_labels = [\"Left_Hand\", \"Right_Hand\", \"Both_Feet\", \"Tongue\"]\n",
    "df_cm = pd.DataFrame(conf_matrix, index=idx_labels, columns = idx_labels)\n",
    "sns.heatmap(df_cm, annot = True, fmt = \"g\")\n",
    "sns.set(font_scale=1.2)\n",
    "plt.title(\"Confussion Matrix (Shallow)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "# Initialize wandb API\n",
    "api = wandb.Api()\n",
    "\n",
    "# Define the project and run ID\n",
    "project_name = \"Master Thesis\"  # Replace with your project name\n",
    "run_id = \"zd2j8s0l\"  # Replace with your specific run ID\n",
    "\n",
    "# Fetch the run from the API\n",
    "run = api.run(f\"philinthesky/{project_name}/{run_id}\")\n",
    "\n",
    "# Retrieve the history of the run (this contains all logged metrics)\n",
    "history = run.history()\n",
    "\n",
    "# Assuming you logged the accuracies with keys \"class_0_accuracy\", \"class_1_accuracy\", etc.\n",
    "# You can adjust the key names based on what you logged\n",
    "class_accuracies = {f'class_{i}': history[f'class_{i}_accuracy'].tolist() for i in range(4)}\n",
    "\n",
    "\n",
    "# If you want to see the accuracy for each class over the epochs\n",
    "simple_dict = {}\n",
    "idx_labels = [\"Left_Hand\", \"Right_Hand\", \"Both_Feet\", \"Tongue\"]\n",
    "for number, (class_name, accuracies) in enumerate(class_accuracies.items()):\n",
    "    adf_result = adfuller(accuracies[0:-1])\n",
    "    print(f\"{class_name}: {adf_result}\")\n",
    "    print(len(accuracies[0:-1]))\n",
    "    simple_dict[idx_labels[number]] = accuracies[0:-1]\n",
    "    #print(f'ADF Statistic: {adf_result[0]}')\n",
    "    #print(f'p-value: {adf_result[1]}')\n",
    "    #print(f\"{class_name}: {accuracies}\")\n",
    "    #data = ???\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_labels = [\"Left_Hand\", \"Right_Hand\", \"Both_Feet\", \"Tongue\"]\n",
    "for count, (class_name, accuracies) in enumerate(class_accuracies.items()):\n",
    "    print(count)\n",
    "     # Convert accuracies to a pandas Series for easier handling\n",
    "    accuracies_series = pd.Series(accuracies)\n",
    "    nan_indices = np.where(np.isnan(accuracies))[0]\n",
    "    print(nan_indices)\n",
    "    # Check for NaN values\n",
    "    nan_values = accuracies_series.isna().sum()\n",
    "    \n",
    "    # Check for Inf values\n",
    "    inf_values = np.isinf(accuracies_series).sum()\n",
    "    \n",
    "    print(f\"{idx_labels[count]}: NaN values = {nan_values}, Inf values = {inf_values}\")\n",
    "\n",
    "    # Plot the accuracies (if no NaN or Inf values)\n",
    "    if nan_values == 0 and inf_values == 0:\n",
    "        plt.plot(accuracies[0:-1], label=idx_labels[count])\n",
    "    plt.plot(accuracies, label=idx_labels[count])\n",
    "\n",
    "plt.ylabel(\"Accuracies\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.title(\"Shallow\")\n",
    "plt.legend(fontsize = 8)\n",
    "plt.show()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the index labels (classes)\n",
    "idx_labels = [\"Left_Hand\", \"Right_Hand\", \"Both_Feet\", \"Tongue\"]\n",
    "# Adjust the row index to represent time steps (100 data points)\n",
    "time_steps = [f\"Time Step {i+1}\" for i in range(200)]  # Creates a list of time steps from \"Time Step 1\" to \"Time Step 100\"\n",
    "\n",
    "# Construct the DataFrame with 100 rows (time steps) and 4 columns (for each class)\n",
    "df_simple = pd.DataFrame(simple_dict, index=time_steps)\n",
    "\n",
    "# Check the DataFrame\n",
    "print(df_simple.head(10))  # Print first 5 rows to check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from statsmodels.tsa.api import VAR\n",
    "\n",
    "def select_lag_bic(data, max_lags=12):\n",
    "    \"\"\"\n",
    "    Selects the optimal lag based on Bayesian Information Criterion (BIC).\n",
    "    \n",
    "    Parameters:\n",
    "        data (pd.DataFrame): Time series data with multiple variables (columns).\n",
    "        max_lags (int): Maximum number of lags to evaluate.\n",
    "    \n",
    "    Returns:\n",
    "        int: Optimal lag length based on BIC.\n",
    "    \"\"\"\n",
    "    model = VAR(data)\n",
    "    bic_values = []\n",
    "    \n",
    "    for lag in range(1, max_lags + 1):\n",
    "        result = model.fit(lag)\n",
    "        bic_values.append((lag, result.bic))\n",
    "\n",
    "    # Find the lag with the minimum BIC value\n",
    "    optimal_lag = min(bic_values, key=lambda x: x[1])[0]\n",
    "\n",
    "    return optimal_lag\n",
    "\n",
    "# Example usage:\n",
    "optimal_lag = select_lag_bic(df_simple)  # df_simple is your DataFrame\n",
    "print(f\"Optimal number of lags based on BIC: {optimal_lag}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.stattools import grangercausalitytests\n",
    "maxlag = 1\n",
    "def grangers_causation_matrix(data, variables, test='ssr_chi2test', verbose=False):    \n",
    "    \"\"\"Check Granger Causality of all possible combinations of the Time series.\n",
    "    The rows are the response variable, columns are predictors. The values in the table \n",
    "    are the P-Values. P-Values lesser than the significance level (0.05), implies \n",
    "    the Null Hypothesis that the coefficients of the corresponding past values is \n",
    "    zero, that is, the X does not cause Y can be rejected.\n",
    "\n",
    "    data      : pandas dataframe containing the time series variables\n",
    "    variables : list containing names of the time series variables.\n",
    "    \"\"\"\n",
    "    df = pd.DataFrame(np.zeros((len(variables), len(variables))), columns=variables, index=variables)\n",
    "    for c in df.columns:\n",
    "        for r in df.index:\n",
    "            test_result = grangercausalitytests(data[[r, c]], maxlag=maxlag, verbose=False)\n",
    "            p_values = [round(test_result[i+1][0][test][1],4) for i in range(maxlag)]\n",
    "            if verbose: print(f'Y = {r}, X = {c}, P Values = {p_values}')\n",
    "            min_p_value = np.min(p_values)\n",
    "            df.loc[r, c] = min_p_value\n",
    "    df.columns = [var + '_x' for var in variables]\n",
    "    df.index = [var + '_y' for var in variables]\n",
    "    return df\n",
    "\n",
    "\n",
    "df_granger_causation_matrix = grangers_causation_matrix(df_simple, variables = df_simple.columns)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_granger_causation_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_granger_causation_matrix\n",
    "sns.heatmap(df_granger_causation_matrix, annot = True)#, fmt = \"g\")\n",
    "plt.title(\"Granger Causation Matrix (Conformer)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
