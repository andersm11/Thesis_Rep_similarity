{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from shallow_fbcsp import ShallowFBCSPNet\n",
    "import pandas as pd\n",
    "from collections import OrderedDict\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_kernel(X):\n",
    "    \"\"\"Computes the linear kernel matrix for X.\"\"\"\n",
    "    return torch.matmul(X,X.T)  # Dot product\n",
    "\n",
    "def centering_matrix(K):\n",
    "    \"\"\"Apply centering to the kernel matrix.\"\"\"\n",
    "    n = K.shape[0]\n",
    "    H = torch.eye(n) - (1.0 / n) * torch.ones((n, n), device=K.device)\n",
    "    return H @ K @ H  # Centered kernel matrix\n",
    "\n",
    "def compute_hsic(K_x, K_y):\n",
    "    \"\"\"\n",
    "    Computes the Hilbert-Schmidt Independence Criterion (HSIC).\n",
    "    \n",
    "    Parameters:\n",
    "    - X: (n_samples, n_features_X) numpy array\n",
    "    - Y: (n_samples, n_features_Y) numpy array\n",
    "    - kernel_X: function to compute the kernel matrix for X\n",
    "    - kernel_Y: function to compute the kernel matrix for Y\n",
    "    \n",
    "    Returns:\n",
    "    - HSIC value (float)\n",
    "    \"\"\"\n",
    "    K_x_centered = centering_matrix(K_x)\n",
    "    K_y_centered = centering_matrix(K_y)\n",
    "    hsic_value = np.trace(K_x_centered @ K_y_centered) / ((K_x.shape[0] - 1) ** 2)\n",
    "    return hsic_value\n",
    "  \n",
    "def compute_CKA(K_x,K_y):\n",
    "  \"\"\"\n",
    "  compute CKA between two X,Y activations\n",
    "  \n",
    "  Parameters:\n",
    "  - X: (n_samples, x_features)\n",
    "  - Y: (n_samples, y_features)\n",
    "  - kernel_X: kernel for X\n",
    "  - kernel_Y: kernel for Y  \n",
    "  \"\"\"\n",
    "  HSIC_KL = compute_hsic(K_x,K_y) \n",
    "  HSIC_KK = compute_hsic(K_x,K_x)\n",
    "  HSIC_LL = compute_hsic(K_y,K_y)\n",
    "  numerator = HSIC_KL\n",
    "  denominator = math.sqrt(HSIC_KK * HSIC_LL)\n",
    "  return(numerator/denominator).item()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model parameters\n",
    "in_chans = 22\n",
    "n_classes = 4\n",
    "n_channels = 22\n",
    "input_window_samples = 1125\n",
    "# Load two models for comparison\n",
    "model= torch.load(\"braindecode_model_temponly_1.pth\",weights_only = False,map_location=torch.device('cpu'))\n",
    "model2 =ShallowFBCSPNet(in_chans, n_classes, input_window_samples)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_model_activations(model, input_tensor):\n",
    "    activations = OrderedDict()\n",
    "\n",
    "    def get_activation(name):\n",
    "        def hook(model, input, output):\n",
    "            activations[name] = output.detach()\n",
    "        return hook\n",
    "\n",
    "    # Register hooks\n",
    "    for name, layer in model.named_modules():\n",
    "        if(name == \"conv_time\" or name == \"conv_spat\"):\n",
    "            layer.register_forward_hook(get_activation(name))\n",
    "\n",
    "\n",
    "    # Run forward pass AFTER registering hooks\n",
    "    model.eval()\n",
    "   \n",
    "    _ = model(input_tensor)  \n",
    "\n",
    "    return activations  # Return collected activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "\n",
    "def extract_model_activations(model, input_tensor, output_dir, batch_size=128):\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "\n",
    "    activations = OrderedDict()\n",
    "\n",
    "    def get_activation(name):\n",
    "        def hook(model, input, output):\n",
    "            activations[name] = output.detach()\n",
    "        return hook\n",
    "\n",
    "    # Register hooks for specific layers\n",
    "    for name, layer in model.named_modules():\n",
    "        if name == \"conv_time\" or name == \"conv_spat\":  # Modify as per your layer names\n",
    "            layer.register_forward_hook(get_activation(name))\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i in range(0, input_tensor.shape[0], batch_size):\n",
    "            batch = input_tensor[i:i + batch_size]  # Select current batch\n",
    "            _ = model(batch)  # Forward pass through the model\n",
    "\n",
    "            # Save activations after each batch\n",
    "            for name, activation in activations.items():\n",
    "                batch_idx = i // batch_size + 1  # This determines the batch number\n",
    "                print(f\"saving: {name}_batch_{batch_idx}.pt\")\n",
    "                torch.save(activation, os.path.join(output_dir, f\"{name}_batch_{batch_idx}.pt\"))\n",
    "            \n",
    "            # Clear activations list after saving\n",
    "            activations.clear()\n",
    "            torch.cuda.empty_cache()  # Optional: Clear GPU memory after each batch\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('Datasets/test_set.pkl', 'rb') as f:\n",
    "    test_set = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.stack([torch.from_numpy(test_set[i][0]) for i in range(len(test_set))])\n",
    "\n",
    "total_samples = X.shape[0]\n",
    "print(X.shape)  # Verify the tensor shape\n",
    "print(type(X))  # Should output <class 'torch.Tensor'>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.conv_time.weight.device)  # Check the device of the conv_time layer\n",
    "print(next(model.parameters()).device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "batch_size = 100\n",
    "total_samples = X.shape[0]\n",
    "print(total_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_model_activations(model,X,output_dir=\"Datasets/activations/model1/\",batch_size=batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_model_activations(model2,X,output_dir=\"Datasets/activations/model2/\",batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "length = torch.load(f\"Datasets/activations/model1/conv_time_batch_{3}.pt\")\n",
    "print(length.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import OrderedDict\n",
    "del X\n",
    "# Use OrderedDict to preserve the order of layer names\n",
    "layer_names_model1 = ['conv_time', 'conv_spat']\n",
    "layer_names_model2 = layer_names_model1\n",
    "\n",
    "\n",
    "print(layer_names_model1)\n",
    "print(layer_names_model2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "batch_nr = total_samples /batch_size\n",
    "\n",
    "batch_nr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Compute CKA for each layer from all activations\n",
    "\n",
    "\n",
    "\n",
    "def compute_kernel_full_highmem(layer, batch_nr, model_number):\n",
    "    \"\"\"Computes the linear kernel for all samples at once.\"\"\"\n",
    "    \n",
    "    # List to hold activations for all batches\n",
    "    activations_list = []\n",
    "    \n",
    "    for batch_idx in range(1, math.ceil(batch_nr+1)):\n",
    "        print(f\"\\rLoading batch {batch_idx} for {layer} in model {model_number}\", end='', flush=True)\n",
    "        \n",
    "        # Load batch activations and flatten them\n",
    "        batch_activations = torch.load(f\"Datasets/activations/model{model_number}/{layer}_batch_{batch_idx}.pt\")\n",
    "        batch_activations = batch_activations.reshape(batch_activations.shape[0], -1)  # Flatten\n",
    "        \n",
    "        activations_list.append(batch_activations)\n",
    "    \n",
    "    # Concatenate all activations from batches along the 0th axis (samples)\n",
    "    all_activations = torch.cat(activations_list, dim=0)\n",
    "    print(\"\\nFinal Activations Shape:\", all_activations.shape)\n",
    "\n",
    "    # Compute the kernel for all samples at once (linear kernel)\n",
    "    full_kernel = torch.matmul(all_activations, all_activations.T)\n",
    "\n",
    "    return full_kernel\n",
    "\n",
    "model_1_kernels = OrderedDict()\n",
    "model_2_kernels = OrderedDict()\n",
    "\n",
    "for layer in layer_names_model1:\n",
    "    model_1_kernels[layer] = compute_kernel_full_highmem(layer, batch_nr,1)\n",
    "for layer in layer_names_model2:\n",
    "    model_2_kernels[layer] = compute_kernel_full_highmem(layer,batch_nr,2)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(model_1_kernels[\"conv_time\"].shape)\n",
    "if (model_1_kernels[\"conv_time\"] == 0).any():\n",
    "    print(\"The tensor contains at least one zero value.\")\n",
    "else:\n",
    "    print(\"No zero values in the tensor.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cka_results = OrderedDict()\n",
    "\n",
    "for layer1, K_x in model_1_kernels.items():\n",
    "    for layer2, K_y in model_2_kernels.items():\n",
    "        cka_value = compute_CKA(K_x, K_y)\n",
    "        cka_results[(layer1, layer2)] = cka_value\n",
    "        print(f\"CKA({layer1}, {layer2}): {cka_value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "n_layers = len(layer_names_model1)# + len(layer_names_model2)\n",
    "matrix = np.zeros((n_layers, n_layers))\n",
    "\n",
    "\n",
    "for (layer1, layer2), similarity in cka_results.items():\n",
    "    i = layer_names_model1.index(layer1) if layer1 in layer_names_model1 else len(layer_names_model1) + layer_names_model2.index(layer1)\n",
    "    j = layer_names_model2.index(layer2) if layer2 in layer_names_model2 else len(layer_names_model2) + layer_names_model1.index(layer2)\n",
    "    \n",
    "    matrix[i, j] = similarity\n",
    "    matrix[j, i] = similarity  # Symmetric matrix\n",
    "\n",
    "df = pd.DataFrame(matrix, index=layer_names_model1, columns=layer_names_model1)\n",
    "\n",
    "# Plot the heatmap\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(df, annot=True, cmap='magma', fmt='.2f', square=True, linewidths=0.5, cbar=True, vmin=0, vmax=1)\n",
    "plt.title('Average CKA Similarity Heatmap')\n",
    "plt.xlabel('Layer')\n",
    "plt.ylabel('Layer')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_kernel_full_lowmem(layer, batch_nr, model_number, total_samples,batch_size, use_cuda=False):\n",
    "    \"\"\"Computes the full kernel matrix in batches efficiently using matrix multiplication.\"\"\"\n",
    "    \n",
    "    device = torch.device(\"cuda\" if use_cuda and torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "\n",
    "    full_kernel = torch.zeros((total_samples, total_samples), dtype=torch.float32, device=device)\n",
    "    \n",
    "\n",
    "    for batch_idx in range(1, math.ceil(batch_nr + 1)):\n",
    "        print(f\"\\rLoading batch {batch_idx} for {layer} in model {model_number}\", end='', flush=True)\n",
    "        \n",
    "        batch_activations = torch.load(f\"Datasets/activations/model{model_number}/{layer}_batch_{batch_idx}.pt\").to(device)\n",
    "        batch_activations = batch_activations.reshape(batch_activations.shape[0], -1)\n",
    "        \n",
    "        start_idx_col = (batch_idx - 1) * batch_size\n",
    "        end_idx_col = min(start_idx_col + batch_size, total_samples)  # Fix end index\n",
    "        for batch_idx2 in range(1, math.ceil(batch_nr + 1)):  # Compute only lower triangle\n",
    "            batch_activations_transpose = torch.load(\n",
    "                f\"Datasets/activations/model{model_number}/{layer}_batch_{batch_idx2}.pt\"\n",
    "            ).to(device)\n",
    "            batch_activations_transpose = batch_activations_transpose.reshape(batch_activations_transpose.shape[0], -1)\n",
    "            \n",
    "            start_idx_row = (batch_idx2 - 1) * batch_size\n",
    "            end_idx_row = min(start_idx_row + batch_size, total_samples)  # Fix end index\n",
    "\n",
    "            kernel_block = batch_activations @ batch_activations_transpose.T  # Matrix multiplication\n",
    "            full_kernel[start_idx_col:end_idx_col, start_idx_row:end_idx_row] = kernel_block\n",
    "            full_kernel[start_idx_row:end_idx_row, start_idx_col:end_idx_col] = kernel_block.T  # Use symmetry\n",
    "    return full_kernel.cpu()\n",
    "\n",
    "\n",
    "def compute_full_kernels(layer_names_model1, layer_names_model2, batch_nr, total_samples,batch_size):\n",
    "    \"\"\"Computes the kernels for both models.\"\"\"\n",
    "    model_1_kernels = {}\n",
    "    model_2_kernels = {}\n",
    "\n",
    "    # Compute kernels for model 1\n",
    "    for layer in layer_names_model1:\n",
    "        model_1_kernels[layer] = compute_kernel_full_lowmem(layer, batch_nr, 1, total_samples,batch_size)\n",
    "\n",
    "    # Compute kernels for model 2\n",
    "    for layer in layer_names_model2:\n",
    "        model_2_kernels[layer] = compute_kernel_full_lowmem(layer, batch_nr, 2, total_samples,batch_size)\n",
    "\n",
    "    return model_1_kernels, model_2_kernels\n",
    "\n",
    "model_1_kernels = OrderedDict()\n",
    "model_2_kernels = OrderedDict()\n",
    "layer_names_model = layer_names_model1\n",
    "\n",
    "    \n",
    "model_1_kernels, model_2_kernels = compute_full_kernels(layer_names_model, layer_names_model, batch_nr,total_samples,batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(model_1_kernels[\"conv_time\"].shape)\n",
    "if (model_1_kernels[\"conv_time\"] == 0).any():\n",
    "    print(\"The tensor contains at least one zero value.\")\n",
    "else:\n",
    "    print(\"No zero values in the tensor.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cka_results = OrderedDict()\n",
    "\n",
    "for layer1, K_x in model_1_kernels.items():\n",
    "    for layer2, K_y in model_2_kernels.items():\n",
    "        cka_value = compute_CKA(K_x, K_y)\n",
    "        cka_results[(layer1, layer2)] = cka_value\n",
    "        print(f\"CKA({layer1}, {layer2}): {cka_value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a square matrix for the CKA similarities\n",
    "n_layers = len(layer_names_model1)# + len(layer_names_model2)\n",
    "matrix = np.zeros((n_layers, n_layers))\n",
    "\n",
    "# Fill the matrix with the average CKA similarity values\n",
    "for (layer1, layer2), similarity in cka_results.items():\n",
    "    i = layer_names_model1.index(layer1) if layer1 in layer_names_model1 else len(layer_names_model1) + layer_names_model2.index(layer1)\n",
    "    j = layer_names_model2.index(layer2) if layer2 in layer_names_model2 else len(layer_names_model2) + layer_names_model1.index(layer2)\n",
    "    \n",
    "    matrix[i, j] = similarity\n",
    "    matrix[j, i] = similarity  # Symmetric matrix\n",
    "\n",
    "# Create a DataFrame for better visualization\n",
    "df = pd.DataFrame(matrix, index=layer_names_model1, columns=layer_names_model1)\n",
    "\n",
    "# Plot the heatmap\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(df, annot=True, cmap='magma', fmt='.2f', square=True, linewidths=0.5, cbar=True, vmin=0, vmax=1)\n",
    "plt.title('Average CKA Similarity Heatmap')\n",
    "plt.xlabel('Layer')\n",
    "plt.ylabel('Layer')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
