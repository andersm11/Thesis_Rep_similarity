{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import importlib\n",
    "import subprocess\n",
    "\n",
    "def install_if_colab():\n",
    "    if \"google.colab\" in sys.modules:\n",
    "        print(\"Running in Google Colab. Checking required libraries...\")\n",
    "\n",
    "        packages = [\"torch\", \"numpy\", \"matplotlib\"]  # Add required libraries\n",
    "        missing_packages = [pkg for pkg in packages if importlib.util.find_spec(pkg) is None]\n",
    "\n",
    "        if missing_packages:\n",
    "            print(f\"Installing missing libraries: {', '.join(missing_packages)}\")\n",
    "            !pip install {\" \".join(missing_packages)}\n",
    "        else:\n",
    "            print(\"All required libraries are already installed.\")\n",
    "    else:\n",
    "        print(\"Not running in Google Colab. No installation needed.\")\n",
    "\n",
    "install_if_colab()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from braindecode.datasets import MOABBDataset\n",
    "\n",
    "subject_id = [1,2,3,4]\n",
    "dataset = MOABBDataset(dataset_name=\"BNCI2014_001\", subject_ids=[1,2,3,4,5,6,7,8,9])\n",
    "\n",
    "\n",
    "\n",
    "from braindecode.preprocessing import (\n",
    "    exponential_moving_standardize,\n",
    "    preprocess,\n",
    "    Preprocessor,\n",
    ")\n",
    "\n",
    "low_cut_hz = 4.0  # low cut frequency for filtering\n",
    "high_cut_hz = 38.0  # high cut frequency for filtering\n",
    "# Parameters for exponential moving standardization\n",
    "factor_new = 1e-3\n",
    "init_block_size = 1000\n",
    "\n",
    "transforms = [\n",
    "    Preprocessor(\"pick_types\", eeg=True, meg=False, stim=False),  # Keep EEG sensors\n",
    "    Preprocessor(\n",
    "        lambda data, factor: np.multiply(data, factor),  # Convert from V to uV\n",
    "        factor=1e6,\n",
    "    ),\n",
    "    Preprocessor(\"filter\", l_freq=low_cut_hz, h_freq=high_cut_hz),  # Bandpass filter\n",
    "    Preprocessor(\n",
    "        exponential_moving_standardize,  # Exponential moving standardization\n",
    "        factor_new=factor_new,\n",
    "        init_block_size=init_block_size,\n",
    "    ),\n",
    "]\n",
    "\n",
    "# Transform the data\n",
    "preprocess(dataset, transforms, n_jobs=-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from braindecode.preprocessing import create_windows_from_events\n",
    "\n",
    "trial_start_offset_seconds = -0.5\n",
    "# Extract sampling frequency, check that they are same in all datasets\n",
    "sfreq = dataset.datasets[0].raw.info[\"sfreq\"]\n",
    "assert all([ds.raw.info[\"sfreq\"] == sfreq for ds in dataset.datasets])\n",
    "# Calculate the trial start offset in samples.\n",
    "trial_start_offset_samples = int(trial_start_offset_seconds * sfreq)\n",
    "\n",
    "# Create windows using braindecode function for this. It needs parameters to define how\n",
    "# trials should be used.\n",
    "windows_dataset = create_windows_from_events(\n",
    "    dataset,\n",
    "    trial_start_offset_samples=trial_start_offset_samples,\n",
    "    trial_stop_offset_samples=0,\n",
    "    preload=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collapsed_shallow_fbscp as c_shallow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "#from shallow_fbcsp import ShallowFBCSPNet\n",
    "from braindecode.util import set_random_seeds\n",
    "\n",
    "\n",
    "cuda = torch.cuda.is_available()  # check if GPU is available, if True chooses to use it\n",
    "device = \"cuda\" if cuda else \"cpu\"\n",
    "if cuda:\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "seed = 20200222\n",
    "set_random_seeds(seed=seed, cuda=cuda)\n",
    "\n",
    "n_classes = 4\n",
    "classes = list(range(n_classes))\n",
    "# Extract number of chans and time steps from dataset\n",
    "n_channels = windows_dataset[0][0].shape[0]\n",
    "input_window_samples = windows_dataset[0][0].shape[1]\n",
    "\n",
    "print(\"n_classes: \", n_classes)\n",
    "print(\"n_channels:\", n_channels)\n",
    "print(\"input_window_samples size:\", input_window_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#windows_dataset[0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!dir collapsed_shallow_fbscp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from models_fbscp import CollapsedShallowNet\n",
    "# The ShallowFBCSPNet is a `nn.Sequential` model\n",
    "from shallow_laurits import ShallowFBCSPNet\n",
    "model = ShallowFBCSPNet(\n",
    "    n_chans=22,\n",
    "    n_outputs=n_classes,\n",
    "    n_times=input_window_samples,\n",
    "    final_conv_length=\"auto\",\n",
    ")\n",
    "\n",
    "# Display torchinfo table describing the model\n",
    "print(model)\n",
    "\n",
    "# Send model to GPU\n",
    "if cuda:\n",
    "    model.cuda()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "splitted = windows_dataset.split(\"session\")\n",
    "train_set = splitted['0train']  # Session train\n",
    "test_set = splitted['1test']  # Session evaluation\n",
    "\n",
    "from torch.nn import Module\n",
    "from torch.optim.lr_scheduler import LRScheduler\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "#lr = 1e-4\n",
    "#weight_decay = 1e-4\n",
    "#batch_size = 64\n",
    "#n_epochs = 200\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True)\n",
    "#progress_bar = tqdm(enumerate(train_loader), total=len(train_loader))\n",
    "\n",
    "\n",
    "#from collections import defaultdict\n",
    "\n",
    "#counting_dict = defaultdict(int)  # Initialize class counter\n",
    "\n",
    "#for batch_idx, (X, y, _) in progress_bar:\n",
    "#   X, y = X.to(device), y.to(device)  # Move to device if needed\n",
    "    \n",
    "    # Count occurrences of each class in y\n",
    "#    for value in y:\n",
    "#        counting_dict[int(value.item())] += 1  # Convert tensor to int and update count\n",
    "\n",
    "# Print class frequencies\n",
    "#print(\"Class counts:\", dict(counting_dict))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from tqdm import tqdm\n",
    "# Define a method for training one epoch\n",
    "\n",
    "\n",
    "def train_one_epoch(\n",
    "        dataloader: DataLoader, model: Module, loss_fn, optimizer,\n",
    "        scheduler: LRScheduler, epoch: int, device, print_batch_stats=True\n",
    "):\n",
    "    model.train()  # Set the model to training mode\n",
    "    train_loss, correct = 0, 0\n",
    "\n",
    "    progress_bar = tqdm(enumerate(dataloader), total=len(dataloader),\n",
    "                        disable=not print_batch_stats)\n",
    "\n",
    "    for batch_idx, (X, y, _) in progress_bar:\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        #print(X.shape)\n",
    "        optimizer.zero_grad()\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()  # update the model weights\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        correct += (pred.argmax(1) == y).sum().item()\n",
    "\n",
    "        #if print_batch_stats:\n",
    "        #    progress_bar.set_description(\n",
    "        #        f\"Epoch {epoch}/{n_epochs}, \"\n",
    "        #        f\"Batch {batch_idx + 1}/{len(dataloader)}, \"\n",
    "        #        f\"Loss: {loss.item():.6f}\"\n",
    "        #    )\n",
    "\n",
    "    # Update the learning rate\n",
    "    scheduler.step()\n",
    "\n",
    "    correct /= len(dataloader.dataset)\n",
    "    return train_loss / len(dataloader), correct\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import numpy as np\n",
    "\n",
    "@torch.no_grad()\n",
    "def test_model(dataloader: DataLoader, model: torch.nn.Module, loss_fn, print_batch_stats=True):\n",
    "    device = next(model.parameters()).device  # Get model device\n",
    "    size = len(dataloader.dataset)\n",
    "    n_batches = len(dataloader)\n",
    "    model.eval()  # Switch to evaluation mode\n",
    "    test_loss, correct = 0, 0\n",
    "\n",
    "    # Initialize dictionaries for per-class tracking\n",
    "    class_correct = defaultdict(int)\n",
    "    class_total = defaultdict(int)\n",
    "\n",
    "    # Lists to store true and predicted labels for confusion matrix\n",
    "    all_preds = []\n",
    "    all_targets = []\n",
    "\n",
    "    if print_batch_stats:\n",
    "        progress_bar = tqdm(enumerate(dataloader), total=len(dataloader))\n",
    "    else:\n",
    "        progress_bar = enumerate(dataloader)\n",
    "\n",
    "    for batch_idx, (X, y, _) in progress_bar:\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        pred = model(X)\n",
    "        batch_loss = loss_fn(pred, y).item()\n",
    "\n",
    "        test_loss += batch_loss\n",
    "        correct += (pred.argmax(1) == y).sum().item()\n",
    "\n",
    "        # Store predictions and true labels for confusion matrix\n",
    "        all_preds.append(pred.argmax(1).cpu())\n",
    "        all_targets.append(y.cpu())\n",
    "\n",
    "        # Compute per-class accuracy\n",
    "        preds_labels = pred.argmax(1)\n",
    "        for label, pred_label in zip(y, preds_labels):\n",
    "            class_total[label.item()] += 1\n",
    "            class_correct[label.item()] += (label == pred_label).item()\n",
    "\n",
    "        if print_batch_stats:\n",
    "            progress_bar.set_description(\n",
    "                f\"Batch {batch_idx + 1}/{len(dataloader)}, Loss: {batch_loss:.6f}\"\n",
    "            )\n",
    "\n",
    "    # Convert lists to tensors\n",
    "    all_preds = torch.cat(all_preds)\n",
    "    all_targets = torch.cat(all_targets)\n",
    "\n",
    "    # Compute per-class accuracy\n",
    "    class_accuracies = {\n",
    "        cls: (class_correct[cls] / class_total[cls]) * 100 if class_total[cls] > 0 else 0\n",
    "        for cls in class_total\n",
    "    }\n",
    "\n",
    "    # Compute overall accuracy\n",
    "    test_loss /= n_batches\n",
    "    overall_accuracy = (correct / size) * 100\n",
    "\n",
    "    # Print per-class accuracy\n",
    "    print(\"\\nClass-wise Accuracy:\")\n",
    "    for cls, acc in class_accuracies.items():\n",
    "        print(f\"  Class {cls}: {acc:.2f}%\")\n",
    "\n",
    "    print(f\"Test Accuracy: {overall_accuracy:.1f}%, Test Loss: {test_loss:.6f}\\n\")\n",
    "\n",
    "    return test_loss, overall_accuracy, class_accuracies, all_preds, all_targets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from braindecode.models import ShallowFBCSPNet "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = ShallowFBCSPNet(\n",
    "    n_chans=22,\n",
    "    n_outputs=n_classes,\n",
    "    n_times=input_window_samples,\n",
    "    final_conv_length=\"auto\",\n",
    ")\n",
    "print(model2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import wandb\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from torch.nn import CrossEntropyLoss\n",
    "import numpy as np\n",
    "\n",
    "# Initialize Weights & Biases\n",
    "wandb.init(project=\"Master Thesis\", name=\"Shallow Accuracy\")\n",
    "\n",
    "# Define hyperparameters\n",
    "lr = 0.1\n",
    "weight_decay = 1e-4\n",
    "batch_size = 124  # Start with 124\n",
    "n_epochs = 100\n",
    "\n",
    "# Log hyperparameters to wandb\n",
    "wandb.config.update({\n",
    "    \"learning_rate\": lr,\n",
    "    \"weight_decay\": weight_decay,\n",
    "    \"batch_size\": batch_size,\n",
    "    \"epochs\": n_epochs\n",
    "})\n",
    "\n",
    "# Define optimizer and scheduler\n",
    "optimizer = AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "scheduler = CosineAnnealingLR(optimizer, T_max=n_epochs - 1)\n",
    "\n",
    "# Define loss function\n",
    "loss_fn = CrossEntropyLoss()\n",
    "\n",
    "# Create DataLoaders\n",
    "train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_set, batch_size=batch_size)\n",
    "\n",
    "# Initialize lists to store all predictions & targets\n",
    "all_preds, all_targets = [], []\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(1, n_epochs + 1):\n",
    "    print(f\"Epoch {epoch}/{n_epochs}: \", end=\"\")\n",
    "\n",
    "    train_loss, train_accuracy = train_one_epoch(\n",
    "        train_loader, model, loss_fn, optimizer, scheduler, epoch, device\n",
    "    )\n",
    "\n",
    "    test_loss, test_accuracy, class_accuracies, batch_preds, batch_targets = test_model(test_loader, model, loss_fn)\n",
    "\n",
    "    # Store predictions & labels for confusion matrix\n",
    "    all_preds.extend(batch_preds)\n",
    "    all_targets.extend(batch_targets)\n",
    "\n",
    "    # Print class-wise accuracy\n",
    "    print(\"\\nClass-wise Accuracy:\")\n",
    "    for class_idx, acc in class_accuracies.items():\n",
    "        print(f\"  Class {class_idx}: {acc:.2f}%\")\n",
    "\n",
    "    # Log results to wandb\n",
    "    wandb.log({\n",
    "        \"epoch\": epoch,\n",
    "        \"train_loss\": train_loss,\n",
    "        \"train_accuracy\": train_accuracy * 100,\n",
    "        \"test_loss\": test_loss,\n",
    "        \"test_accuracy\": test_accuracy,\n",
    "        \"learning_rate\": scheduler.get_last_lr()[0],\n",
    "        **{f\"class_{class_idx}_accuracy\": acc for class_idx, acc in class_accuracies.items()}\n",
    "    })\n",
    "\n",
    "    print(\n",
    "        f\"Train Accuracy: {100 * train_accuracy:.2f}%, \"\n",
    "        f\"Average Train Loss: {train_loss:.6f}, \"\n",
    "        f\"Test Accuracy: {test_accuracy:.2f}%, \"\n",
    "        f\"Average Test Loss: {test_loss:.6f}\\n\"\n",
    "    )\n",
    "\n",
    "# Convert lists to NumPy arrays\n",
    "all_preds = np.array(all_preds)\n",
    "all_targets = np.array(all_targets)\n",
    "\n",
    "# Save predictions & true labels for later use (confusion matrix)\n",
    "wandb.log({\"all_preds\": all_preds.tolist(), \"all_targets\": all_targets.tolist()})\n",
    "\n",
    "wandb.finish()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Load predictions and true labels (ensure these are NumPy arrays)\n",
    "all_preds = np.array(wandb.run.history(keys=[\"all_preds\"])).flatten()\n",
    "all_targets = np.array(wandb.run.history(keys=[\"all_targets\"])).flatten()\n",
    "\n",
    "# Compute confusion matrix\n",
    "cm = confusion_matrix(all_targets, all_preds)\n",
    "\n",
    "# Define class labels (modify if needed)\n",
    "class_labels = [f\"Class {i}\" for i in range(cm.shape[0])]\n",
    "\n",
    "# Plot confusion matrix using seaborn\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=class_labels, yticklabels=class_labels)\n",
    "plt.xlabel(\"Predicted Label\")\n",
    "plt.ylabel(\"True Label\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "# Assuming 'model' is your trained Braindecode model\n",
    "torch.save(model, \"braindecode_model_temponly.pth\")\n",
    "torch.save(model.state_dict(), \"braindecode_model_temponly_state.pth\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
