{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\miniconda3\\lib\\site-packages\\braindecode\\preprocessing\\preprocess.py:55: UserWarning: Preprocessing choices with lambda functions cannot be saved.\n",
      "  warn('Preprocessing choices with lambda functions cannot be saved.')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<braindecode.datasets.moabb.MOABBDataset at 0x22b94bdc3d0>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from braindecode.datasets import MOABBDataset\n",
    "\n",
    "subject_id = [1,2,3,4]\n",
    "dataset = MOABBDataset(dataset_name=\"BNCI2014_001\", subject_ids=[1,2,3,4,5,6,7,8,9])\n",
    "\n",
    "\n",
    "\n",
    "from braindecode.preprocessing import (\n",
    "    exponential_moving_standardize,\n",
    "    preprocess,\n",
    "    Preprocessor,\n",
    ")\n",
    "\n",
    "low_cut_hz = 4.0  # low cut frequency for filtering\n",
    "high_cut_hz = 38.0  # high cut frequency for filtering\n",
    "# Parameters for exponential moving standardization\n",
    "factor_new = 1e-3\n",
    "init_block_size = 1000\n",
    "\n",
    "transforms = [\n",
    "    Preprocessor(\"pick_types\", eeg=True, meg=False, stim=False),  # Keep EEG sensors\n",
    "    Preprocessor(\n",
    "        lambda data, factor: np.multiply(data, factor),  # Convert from V to uV\n",
    "        factor=1e6,\n",
    "    ),\n",
    "    Preprocessor(\"filter\", l_freq=low_cut_hz, h_freq=high_cut_hz),  # Bandpass filter\n",
    "    Preprocessor(\n",
    "        exponential_moving_standardize,  # Exponential moving standardization\n",
    "        factor_new=factor_new,\n",
    "        init_block_size=init_block_size,\n",
    "    ),\n",
    "]\n",
    "\n",
    "# Transform the data\n",
    "preprocess(dataset, transforms, n_jobs=-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n"
     ]
    }
   ],
   "source": [
    "from braindecode.preprocessing import create_windows_from_events\n",
    "\n",
    "trial_start_offset_seconds = -0.5\n",
    "# Extract sampling frequency, check that they are same in all datasets\n",
    "sfreq = dataset.datasets[0].raw.info[\"sfreq\"]\n",
    "assert all([ds.raw.info[\"sfreq\"] == sfreq for ds in dataset.datasets])\n",
    "# Calculate the trial start offset in samples.\n",
    "trial_start_offset_samples = int(trial_start_offset_seconds * sfreq)\n",
    "\n",
    "# Create windows using braindecode function for this. It needs parameters to define how\n",
    "# trials should be used.\n",
    "windows_dataset = create_windows_from_events(\n",
    "    dataset,\n",
    "    trial_start_offset_samples=trial_start_offset_samples,\n",
    "    trial_stop_offset_samples=0,\n",
    "    preload=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collapsed_shallow_fbscp as c_shallow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_classes:  4\n",
      "n_channels: 22\n",
      "input_window_samples size: 1125\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from shallow_fbcsp import ShallowFBCSPNet\n",
    "from braindecode.util import set_random_seeds\n",
    "\n",
    "\n",
    "cuda = torch.cuda.is_available()  # check if GPU is available, if True chooses to use it\n",
    "device = \"cuda\" if cuda else \"cpu\"\n",
    "if cuda:\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "seed = 20200222\n",
    "set_random_seeds(seed=seed, cuda=cuda)\n",
    "\n",
    "n_classes = 4\n",
    "classes = list(range(n_classes))\n",
    "# Extract number of chans and time steps from dataset\n",
    "n_channels = windows_dataset[0][0].shape[0]\n",
    "input_window_samples = windows_dataset[0][0].shape[1]\n",
    "\n",
    "print(\"n_classes: \", n_classes)\n",
    "print(\"n_channels:\", n_channels)\n",
    "print(\"input_window_samples size:\", input_window_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#windows_dataset[0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!dir collapsed_shallow_fbscp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================================================================================================\n",
      "Layer (type (var_name):depth-idx)        Input Shape               Output Shape              Param #                   Kernel Shape\n",
      "============================================================================================================================================\n",
      "ShallowFBCSPNet (ShallowFBCSPNet)        [1, 22, 1125]             [1, 4]                    --                        --\n",
      "├─SafeLog (pool_nonlin_exp): 1-1         [1, 22, 1125]             [1, 22, 1125]             --                        --\n",
      "├─Ensure4d (ensuredims): 1-2             [1, 22, 1125]             [1, 22, 1125, 1]          --                        --\n",
      "├─Rearrange (dimshuffle): 1-3            [1, 22, 1125, 1]          [1, 1, 1125, 22]          --                        --\n",
      "├─Conv2d (conv_time): 1-4                [1, 1, 1125, 22]          [1, 40, 1101, 22]         1,040                     [25, 1]\n",
      "├─Conv2d (conv_spat): 1-5                [1, 40, 1101, 22]         [1, 40, 1101, 1]          35,200                    [1, 22]\n",
      "├─BatchNorm2d (bnorm): 1-6               [1, 40, 1101, 1]          [1, 40, 1101, 1]          80                        --\n",
      "├─Expression (conv_nonlin_exp): 1-7      [1, 40, 1101, 1]          [1, 40, 1101, 1]          --                        --\n",
      "├─AvgPool2d (pool): 1-8                  [1, 40, 1101, 1]          [1, 40, 69, 1]            --                        [75, 1]\n",
      "├─SafeLog (pool_nonlin_exp): 1-9         [1, 40, 69, 1]            [1, 40, 69, 1]            --                        --\n",
      "├─Dropout (drop): 1-10                   [1, 40, 69, 1]            [1, 40, 69, 1]            --                        --\n",
      "├─Sequential (final_layer): 1-11         [1, 40, 69, 1]            [1, 4]                    --                        --\n",
      "│    └─Conv2d (conv_classifier): 2-1     [1, 40, 69, 1]            [1, 4, 1, 1]              11,044                    [69, 1]\n",
      "│    └─Expression (squeeze): 2-2         [1, 4, 1, 1]              [1, 4]                    --                        --\n",
      "============================================================================================================================================\n",
      "Total params: 47,364\n",
      "Trainable params: 47,364\n",
      "Non-trainable params: 0\n",
      "Total mult-adds (M): 63.96\n",
      "============================================================================================================================================\n",
      "Input size (MB): 0.10\n",
      "Forward/backward pass size (MB): 8.46\n",
      "Params size (MB): 0.19\n",
      "Estimated Total Size (MB): 8.74\n",
      "============================================================================================================================================\n"
     ]
    }
   ],
   "source": [
    "#from models_fbscp import CollapsedShallowNet\n",
    "# The ShallowFBCSPNet is a `nn.Sequential` model\n",
    "\n",
    "model = ShallowFBCSPNet(\n",
    "    n_chans=22,\n",
    "    n_outputs=n_classes,\n",
    "    n_times=input_window_samples,\n",
    "    final_conv_length=\"auto\",\n",
    ")\n",
    "\n",
    "# Display torchinfo table describing the model\n",
    "print(model)\n",
    "\n",
    "# Send model to GPU\n",
    "if cuda:\n",
    "    model.cuda()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "splitted = windows_dataset.split(\"session\")\n",
    "train_set = splitted['0train']  # Session train\n",
    "test_set = splitted['1test']  # Session evaluation\n",
    "\n",
    "from torch.nn import Module\n",
    "from torch.optim.lr_scheduler import LRScheduler\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "#lr = 1e-4\n",
    "#weight_decay = 1e-4\n",
    "#batch_size = 64\n",
    "#n_epochs = 200\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True)\n",
    "#progress_bar = tqdm(enumerate(train_loader), total=len(train_loader))\n",
    "\n",
    "\n",
    "#from collections import defaultdict\n",
    "\n",
    "#counting_dict = defaultdict(int)  # Initialize class counter\n",
    "\n",
    "#for batch_idx, (X, y, _) in progress_bar:\n",
    "#   X, y = X.to(device), y.to(device)  # Move to device if needed\n",
    "    \n",
    "    # Count occurrences of each class in y\n",
    "#    for value in y:\n",
    "#        counting_dict[int(value.item())] += 1  # Convert tensor to int and update count\n",
    "\n",
    "# Print class frequencies\n",
    "#print(\"Class counts:\", dict(counting_dict))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from tqdm import tqdm\n",
    "# Define a method for training one epoch\n",
    "\n",
    "\n",
    "def train_one_epoch(\n",
    "        dataloader: DataLoader, model: Module, loss_fn, optimizer,\n",
    "        scheduler: LRScheduler, epoch: int, device, print_batch_stats=True\n",
    "):\n",
    "    model.train()  # Set the model to training mode\n",
    "    train_loss, correct = 0, 0\n",
    "\n",
    "    progress_bar = tqdm(enumerate(dataloader), total=len(dataloader),\n",
    "                        disable=not print_batch_stats)\n",
    "\n",
    "    for batch_idx, (X, y, _) in progress_bar:\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        #print(X.shape)\n",
    "        optimizer.zero_grad()\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()  # update the model weights\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        correct += (pred.argmax(1) == y).sum().item()\n",
    "\n",
    "        #if print_batch_stats:\n",
    "        #    progress_bar.set_description(\n",
    "        #        f\"Epoch {epoch}/{n_epochs}, \"\n",
    "        #        f\"Batch {batch_idx + 1}/{len(dataloader)}, \"\n",
    "        #        f\"Loss: {loss.item():.6f}\"\n",
    "        #    )\n",
    "\n",
    "    # Update the learning rate\n",
    "    scheduler.step()\n",
    "\n",
    "    correct /= len(dataloader.dataset)\n",
    "    return train_loss / len(dataloader), correct\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import numpy as np\n",
    "\n",
    "@torch.no_grad()\n",
    "def test_model(dataloader: DataLoader, model: torch.nn.Module, loss_fn, print_batch_stats=True):\n",
    "    device = next(model.parameters()).device  # Get model device\n",
    "    size = len(dataloader.dataset)\n",
    "    n_batches = len(dataloader)\n",
    "    model.eval()  # Switch to evaluation mode\n",
    "    test_loss, correct = 0, 0\n",
    "\n",
    "    # Initialize dictionaries for per-class tracking\n",
    "    class_correct = defaultdict(int)\n",
    "    class_total = defaultdict(int)\n",
    "\n",
    "    # Lists to store true and predicted labels for confusion matrix\n",
    "    all_preds = []\n",
    "    all_targets = []\n",
    "\n",
    "    if print_batch_stats:\n",
    "        progress_bar = tqdm(enumerate(dataloader), total=len(dataloader))\n",
    "    else:\n",
    "        progress_bar = enumerate(dataloader)\n",
    "\n",
    "    for batch_idx, (X, y, _) in progress_bar:\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        pred = model(X)\n",
    "        batch_loss = loss_fn(pred, y).item()\n",
    "\n",
    "        test_loss += batch_loss\n",
    "        correct += (pred.argmax(1) == y).sum().item()\n",
    "\n",
    "        # Store predictions and true labels for confusion matrix\n",
    "        all_preds.append(pred.argmax(1).cpu())\n",
    "        all_targets.append(y.cpu())\n",
    "\n",
    "        # Compute per-class accuracy\n",
    "        preds_labels = pred.argmax(1)\n",
    "        for label, pred_label in zip(y, preds_labels):\n",
    "            class_total[label.item()] += 1\n",
    "            class_correct[label.item()] += (label == pred_label).item()\n",
    "\n",
    "        if print_batch_stats:\n",
    "            progress_bar.set_description(\n",
    "                f\"Batch {batch_idx + 1}/{len(dataloader)}, Loss: {batch_loss:.6f}\"\n",
    "            )\n",
    "\n",
    "    # Convert lists to tensors\n",
    "    all_preds = torch.cat(all_preds)\n",
    "    all_targets = torch.cat(all_targets)\n",
    "\n",
    "    # Compute per-class accuracy\n",
    "    class_accuracies = {\n",
    "        cls: (class_correct[cls] / class_total[cls]) * 100 if class_total[cls] > 0 else 0\n",
    "        for cls in class_total\n",
    "    }\n",
    "\n",
    "    # Compute overall accuracy\n",
    "    test_loss /= n_batches\n",
    "    overall_accuracy = (correct / size) * 100\n",
    "\n",
    "    # Print per-class accuracy\n",
    "    print(\"\\nClass-wise Accuracy:\")\n",
    "    for cls, acc in class_accuracies.items():\n",
    "        print(f\"  Class {cls}: {acc:.2f}%\")\n",
    "\n",
    "    print(f\"Test Accuracy: {overall_accuracy:.1f}%, Test Loss: {test_loss:.6f}\\n\")\n",
    "\n",
    "    return test_loss, overall_accuracy, class_accuracies, all_preds, all_targets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from braindecode.models import ShallowFBCSPNet "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================================================================================================\n",
      "Layer (type (var_name):depth-idx)        Input Shape               Output Shape              Param #                   Kernel Shape\n",
      "============================================================================================================================================\n",
      "ShallowFBCSPNet (ShallowFBCSPNet)        [1, 22, 1125]             [1, 4]                    --                        --\n",
      "├─SafeLog (pool_nonlin_exp): 1-1         [1, 22, 1125]             [1, 22, 1125]             --                        --\n",
      "├─Ensure4d (ensuredims): 1-2             [1, 22, 1125]             [1, 22, 1125, 1]          --                        --\n",
      "├─Rearrange (dimshuffle): 1-3            [1, 22, 1125, 1]          [1, 1, 1125, 22]          --                        --\n",
      "├─Conv2d (conv_time): 1-4                [1, 1, 1125, 22]          [1, 40, 1101, 22]         1,040                     [25, 1]\n",
      "├─Conv2d (conv_spat): 1-5                [1, 40, 1101, 22]         [1, 40, 1101, 1]          35,200                    [1, 22]\n",
      "├─BatchNorm2d (bnorm): 1-6               [1, 40, 1101, 1]          [1, 40, 1101, 1]          80                        --\n",
      "├─Expression (conv_nonlin_exp): 1-7      [1, 40, 1101, 1]          [1, 40, 1101, 1]          --                        --\n",
      "├─AvgPool2d (pool): 1-8                  [1, 40, 1101, 1]          [1, 40, 69, 1]            --                        [75, 1]\n",
      "├─SafeLog (pool_nonlin_exp): 1-9         [1, 40, 69, 1]            [1, 40, 69, 1]            --                        --\n",
      "├─Dropout (drop): 1-10                   [1, 40, 69, 1]            [1, 40, 69, 1]            --                        --\n",
      "├─Sequential (final_layer): 1-11         [1, 40, 69, 1]            [1, 4]                    --                        --\n",
      "│    └─Conv2d (conv_classifier): 2-1     [1, 40, 69, 1]            [1, 4, 1, 1]              11,044                    [69, 1]\n",
      "│    └─Expression (squeeze): 2-2         [1, 4, 1, 1]              [1, 4]                    --                        --\n",
      "============================================================================================================================================\n",
      "Total params: 47,364\n",
      "Trainable params: 47,364\n",
      "Non-trainable params: 0\n",
      "Total mult-adds (M): 63.96\n",
      "============================================================================================================================================\n",
      "Input size (MB): 0.10\n",
      "Forward/backward pass size (MB): 8.46\n",
      "Params size (MB): 0.19\n",
      "Estimated Total Size (MB): 8.74\n",
      "============================================================================================================================================\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================================================================================================\n",
      "Layer (type (var_name):depth-idx)        Input Shape               Output Shape              Param #                   Kernel Shape\n",
      "============================================================================================================================================\n",
      "ShallowFBCSPNet (ShallowFBCSPNet)        [1, 22, 1125]             [1, 4]                    --                        --\n",
      "├─Ensure4d (ensuredims): 1-1             [1, 22, 1125]             [1, 22, 1125, 1]          --                        --\n",
      "├─Rearrange (dimshuffle): 1-2            [1, 22, 1125, 1]          [1, 1, 1125, 22]          --                        --\n",
      "├─CombinedConv (conv_time_spat): 1-3     [1, 1, 1125, 22]          [1, 40, 1101, 1]          36,240                    --\n",
      "├─BatchNorm2d (bnorm): 1-4               [1, 40, 1101, 1]          [1, 40, 1101, 1]          80                        --\n",
      "├─Expression (conv_nonlin_exp): 1-5      [1, 40, 1101, 1]          [1, 40, 1101, 1]          --                        --\n",
      "├─AvgPool2d (pool): 1-6                  [1, 40, 1101, 1]          [1, 40, 69, 1]            --                        [75, 1]\n",
      "├─Expression (pool_nonlin_exp): 1-7      [1, 40, 69, 1]            [1, 40, 69, 1]            --                        --\n",
      "├─Dropout (drop): 1-8                    [1, 40, 69, 1]            [1, 40, 69, 1]            --                        --\n",
      "├─Sequential (final_layer): 1-9          [1, 40, 69, 1]            [1, 4]                    --                        --\n",
      "│    └─Conv2d (conv_classifier): 2-1     [1, 40, 69, 1]            [1, 4, 1, 1]              11,044                    [69, 1]\n",
      "│    └─LogSoftmax (logsoftmax): 2-2      [1, 4, 1, 1]              [1, 4, 1, 1]              --                        --\n",
      "│    └─Expression (squeeze): 2-3         [1, 4, 1, 1]              [1, 4]                    --                        --\n",
      "============================================================================================================================================\n",
      "Total params: 47,364\n",
      "Trainable params: 47,364\n",
      "Non-trainable params: 0\n",
      "Total mult-adds (M): 0.01\n",
      "============================================================================================================================================\n",
      "Input size (MB): 0.10\n",
      "Forward/backward pass size (MB): 0.35\n",
      "Params size (MB): 0.04\n",
      "Estimated Total Size (MB): 0.50\n",
      "============================================================================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\miniconda3\\lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    }
   ],
   "source": [
    "model2 = ShallowFBCSPNet(\n",
    "    n_chans=22,\n",
    "    n_outputs=n_classes,\n",
    "    n_times=input_window_samples,\n",
    "    final_conv_length=\"auto\",\n",
    ")\n",
    "print(model2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d2f8f60431d4170ad5a92c330508a05",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011288888888925108, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Documents\\MsC_ITU\\4th_semester_MSc_ITU\\Master_Thesis\\Thesis_Rep_similarity\\WIP\\wandb\\run-20250220_092042-b00274hj</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/philinthesky/Master%20Thesis/runs/b00274hj' target=\"_blank\">Shallow Accuracy</a></strong> to <a href='https://wandb.ai/philinthesky/Master%20Thesis' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/philinthesky/Master%20Thesis' target=\"_blank\">https://wandb.ai/philinthesky/Master%20Thesis</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/philinthesky/Master%20Thesis/runs/b00274hj' target=\"_blank\">https://wandb.ai/philinthesky/Master%20Thesis/runs/b00274hj</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 21/21 [00:29<00:00,  1.43s/it]\n",
      "Batch 21/21, Loss: 22.575016: 100%|██████████| 21/21 [00:13<00:00,  1.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Class-wise Accuracy:\n",
      "  Class 1: 0.00%\n",
      "  Class 2: 0.00%\n",
      "  Class 0: 0.00%\n",
      "  Class 3: 100.00%\n",
      "Test Accuracy: 25.0%, Test Loss: 25.878341\n",
      "\n",
      "\n",
      "Class-wise Accuracy:\n",
      "  Class 1: 0.00%\n",
      "  Class 2: 0.00%\n",
      "  Class 0: 0.00%\n",
      "  Class 3: 100.00%\n",
      "Train Accuracy: 26.20%, Average Train Loss: 12.436425, Test Accuracy: 25.00%, Average Test Loss: 25.878341\n",
      "\n",
      "Epoch 2/100: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 21/21 [00:28<00:00,  1.35s/it]\n",
      "Batch 21/21, Loss: 11.294884: 100%|██████████| 21/21 [00:09<00:00,  2.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Class-wise Accuracy:\n",
      "  Class 1: 95.99%\n",
      "  Class 2: 4.17%\n",
      "  Class 0: 0.15%\n",
      "  Class 3: 3.70%\n",
      "Test Accuracy: 26.0%, Test Loss: 12.898500\n",
      "\n",
      "\n",
      "Class-wise Accuracy:\n",
      "  Class 1: 95.99%\n",
      "  Class 2: 4.17%\n",
      "  Class 0: 0.15%\n",
      "  Class 3: 3.70%\n",
      "Train Accuracy: 28.09%, Average Train Loss: 6.892707, Test Accuracy: 26.00%, Average Test Loss: 12.898500\n",
      "\n",
      "Epoch 3/100: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 21/21 [00:28<00:00,  1.37s/it]\n",
      "Batch 21/21, Loss: 6.972478: 100%|██████████| 21/21 [00:10<00:00,  1.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Class-wise Accuracy:\n",
      "  Class 1: 15.59%\n",
      "  Class 2: 8.33%\n",
      "  Class 0: 54.63%\n",
      "  Class 3: 20.52%\n",
      "Test Accuracy: 24.8%, Test Loss: 5.471833\n",
      "\n",
      "\n",
      "Class-wise Accuracy:\n",
      "  Class 1: 15.59%\n",
      "  Class 2: 8.33%\n",
      "  Class 0: 54.63%\n",
      "  Class 3: 20.52%\n",
      "Train Accuracy: 32.91%, Average Train Loss: 5.281286, Test Accuracy: 24.77%, Average Test Loss: 5.471833\n",
      "\n",
      "Epoch 4/100: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 21/21 [00:27<00:00,  1.30s/it]\n",
      "Batch 21/21, Loss: 7.674216: 100%|██████████| 21/21 [00:09<00:00,  2.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Class-wise Accuracy:\n",
      "  Class 1: 25.62%\n",
      "  Class 2: 10.03%\n",
      "  Class 0: 41.51%\n",
      "  Class 3: 47.99%\n",
      "Test Accuracy: 31.3%, Test Loss: 4.280886\n",
      "\n",
      "\n",
      "Class-wise Accuracy:\n",
      "  Class 1: 25.62%\n",
      "  Class 2: 10.03%\n",
      "  Class 0: 41.51%\n",
      "  Class 3: 47.99%\n",
      "Train Accuracy: 35.34%, Average Train Loss: 4.554124, Test Accuracy: 31.29%, Average Test Loss: 4.280886\n",
      "\n",
      "Epoch 5/100: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 21/21 [00:28<00:00,  1.37s/it]\n",
      "Batch 21/21, Loss: 6.645712: 100%|██████████| 21/21 [00:09<00:00,  2.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Class-wise Accuracy:\n",
      "  Class 1: 38.58%\n",
      "  Class 2: 3.70%\n",
      "  Class 0: 25.93%\n",
      "  Class 3: 53.70%\n",
      "Test Accuracy: 30.5%, Test Loss: 4.517169\n",
      "\n",
      "\n",
      "Class-wise Accuracy:\n",
      "  Class 1: 38.58%\n",
      "  Class 2: 3.70%\n",
      "  Class 0: 25.93%\n",
      "  Class 3: 53.70%\n",
      "Train Accuracy: 35.73%, Average Train Loss: 4.102095, Test Accuracy: 30.48%, Average Test Loss: 4.517169\n",
      "\n",
      "Epoch 6/100: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 21/21 [00:28<00:00,  1.34s/it]\n",
      "Batch 21/21, Loss: 7.522967: 100%|██████████| 21/21 [00:09<00:00,  2.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Class-wise Accuracy:\n",
      "  Class 1: 36.27%\n",
      "  Class 2: 1.23%\n",
      "  Class 0: 7.25%\n",
      "  Class 3: 76.08%\n",
      "Test Accuracy: 30.2%, Test Loss: 5.290609\n",
      "\n",
      "\n",
      "Class-wise Accuracy:\n",
      "  Class 1: 36.27%\n",
      "  Class 2: 1.23%\n",
      "  Class 0: 7.25%\n",
      "  Class 3: 76.08%\n",
      "Train Accuracy: 39.97%, Average Train Loss: 3.627672, Test Accuracy: 30.21%, Average Test Loss: 5.290609\n",
      "\n",
      "Epoch 7/100: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 21/21 [00:27<00:00,  1.33s/it]\n",
      "Batch 21/21, Loss: 4.260369: 100%|██████████| 21/21 [00:09<00:00,  2.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Class-wise Accuracy:\n",
      "  Class 1: 25.77%\n",
      "  Class 2: 4.63%\n",
      "  Class 0: 17.90%\n",
      "  Class 3: 69.44%\n",
      "Test Accuracy: 29.4%, Test Loss: 4.936643\n",
      "\n",
      "\n",
      "Class-wise Accuracy:\n",
      "  Class 1: 25.77%\n",
      "  Class 2: 4.63%\n",
      "  Class 0: 17.90%\n",
      "  Class 3: 69.44%\n",
      "Train Accuracy: 38.93%, Average Train Loss: 3.522243, Test Accuracy: 29.44%, Average Test Loss: 4.936643\n",
      "\n",
      "Epoch 8/100: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 71%|███████▏  | 15/21 [00:20<00:07,  1.32s/it]"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import wandb\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from torch.nn import CrossEntropyLoss\n",
    "import numpy as np\n",
    "\n",
    "# Initialize Weights & Biases\n",
    "wandb.init(project=\"Master Thesis\", name=\"Shallow Accuracy\")\n",
    "\n",
    "# Define hyperparameters\n",
    "lr = 0.1\n",
    "weight_decay = 1e-4\n",
    "batch_size = 124  # Start with 124\n",
    "n_epochs = 100\n",
    "\n",
    "# Log hyperparameters to wandb\n",
    "wandb.config.update({\n",
    "    \"learning_rate\": lr,\n",
    "    \"weight_decay\": weight_decay,\n",
    "    \"batch_size\": batch_size,\n",
    "    \"epochs\": n_epochs\n",
    "})\n",
    "\n",
    "# Define optimizer and scheduler\n",
    "optimizer = AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "scheduler = CosineAnnealingLR(optimizer, T_max=n_epochs - 1)\n",
    "\n",
    "# Define loss function\n",
    "loss_fn = CrossEntropyLoss()\n",
    "\n",
    "# Create DataLoaders\n",
    "train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_set, batch_size=batch_size)\n",
    "\n",
    "# Initialize lists to store all predictions & targets\n",
    "all_preds, all_targets = [], []\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(1, n_epochs + 1):\n",
    "    print(f\"Epoch {epoch}/{n_epochs}: \", end=\"\")\n",
    "\n",
    "    train_loss, train_accuracy = train_one_epoch(\n",
    "        train_loader, model, loss_fn, optimizer, scheduler, epoch, device\n",
    "    )\n",
    "\n",
    "    test_loss, test_accuracy, class_accuracies, batch_preds, batch_targets = test_model(test_loader, model, loss_fn)\n",
    "\n",
    "    # Store predictions & labels for confusion matrix\n",
    "    all_preds.extend(batch_preds)\n",
    "    all_targets.extend(batch_targets)\n",
    "\n",
    "    # Print class-wise accuracy\n",
    "    print(\"\\nClass-wise Accuracy:\")\n",
    "    for class_idx, acc in class_accuracies.items():\n",
    "        print(f\"  Class {class_idx}: {acc:.2f}%\")\n",
    "\n",
    "    # Log results to wandb\n",
    "    wandb.log({\n",
    "        \"epoch\": epoch,\n",
    "        \"train_loss\": train_loss,\n",
    "        \"train_accuracy\": train_accuracy * 100,\n",
    "        \"test_loss\": test_loss,\n",
    "        \"test_accuracy\": test_accuracy,\n",
    "        \"learning_rate\": scheduler.get_last_lr()[0],\n",
    "        **{f\"class_{class_idx}_accuracy\": acc for class_idx, acc in class_accuracies.items()}\n",
    "    })\n",
    "\n",
    "    print(\n",
    "        f\"Train Accuracy: {100 * train_accuracy:.2f}%, \"\n",
    "        f\"Average Train Loss: {train_loss:.6f}, \"\n",
    "        f\"Test Accuracy: {test_accuracy:.2f}%, \"\n",
    "        f\"Average Test Loss: {test_loss:.6f}\\n\"\n",
    "    )\n",
    "\n",
    "# Convert lists to NumPy arrays\n",
    "all_preds = np.array(all_preds)\n",
    "all_targets = np.array(all_targets)\n",
    "\n",
    "# Save predictions & true labels for later use (confusion matrix)\n",
    "wandb.log({\"all_preds\": all_preds.tolist(), \"all_targets\": all_targets.tolist()})\n",
    "\n",
    "wandb.finish()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Load predictions and true labels (ensure these are NumPy arrays)\n",
    "all_preds = np.array(wandb.run.history(keys=[\"all_preds\"])).flatten()\n",
    "all_targets = np.array(wandb.run.history(keys=[\"all_targets\"])).flatten()\n",
    "\n",
    "# Compute confusion matrix\n",
    "cm = confusion_matrix(all_targets, all_preds)\n",
    "\n",
    "# Define class labels (modify if needed)\n",
    "class_labels = [f\"Class {i}\" for i in range(cm.shape[0])]\n",
    "\n",
    "# Plot confusion matrix using seaborn\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=class_labels, yticklabels=class_labels)\n",
    "plt.xlabel(\"Predicted Label\")\n",
    "plt.ylabel(\"True Label\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.show()\n",
    "`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming 'model' is your trained Braindecode model\n",
    "torch.save(model, \"braindecode_model_temponly.pth\")\n",
    "torch.save(model.state_dict(), \"braindecode_model_temponly_state.pth\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
