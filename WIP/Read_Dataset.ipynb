{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read mbd_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install access_parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!dir v1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from torcheeg.io.eeg_signal import EEGSignalIO\n",
    "\n",
    "## Path to dir with data (remember the last '/')\n",
    "#path = \"/eeg_data/BNCI2014-001/\"\n",
    "path = \"C:/Users/ahmm9/Desktop/v1/\"\n",
    "\n",
    "## Establish connection to datafile\n",
    "IO = EEGSignalIO(io_path=str(path), io_mode='lmdb')\n",
    "\n",
    "## Read metadata dataframe\n",
    "metadata = pd.read_csv(path + 'sample_metadata.tsv', sep='\\t')\n",
    "\n",
    "idxs = np.arange(len(metadata))\n",
    "\n",
    "eeg = torch.FloatTensor(np.array([IO.read_eeg(str(i)) for i in idxs]))\n",
    "print(eeg.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_meta = pd.read_csv(path+\"sample_metadata.tsv\", sep =\"\\t\")\n",
    "\n",
    "newdf = sample_meta.sort_values(\"arousal\")\n",
    "newdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emotion_mapping = {\n",
    "    'sadness': 0, 'tenderness': 1, 'anger': 2, 'amusement': 3, 'joy': 4,\n",
    "    'fear': 5, 'neutral': 6, 'disgust': 7, 'inspiration': 8\n",
    "}\n",
    "\n",
    "stim_valence_mapping = {\n",
    "    'negative': 0, 'neutral': 1, 'positive': 2\n",
    "}\n",
    "\n",
    "# Apply mappings\n",
    "newdf[\"emotion_encoded\"] = newdf[\"emotion\"].map(emotion_mapping)\n",
    "newdf[\"stim_valence_encoded\"] = newdf[\"stim_valence\"].map(stim_valence_mapping)\n",
    "\n",
    "# Print result\n",
    "newdf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newdf.shape[0]/3\n",
    "class_1 = newdf[0:37254]\n",
    "class_2 = newdf[37254:37254*2]\n",
    "class_3 = newdf[37254*2:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "# Split the dataset into three equal parts\n",
    "split_size = newdf.shape[0] // 3\n",
    "class_1 = newdf.iloc[:split_size]\n",
    "class_2 = newdf.iloc[split_size:split_size*2]\n",
    "class_3 = newdf.iloc[split_size*2:]\n",
    "\n",
    "# Define equal bin edges based on the global min/max of arousal\n",
    "arousal_min = newdf[\"arousal\"].min()\n",
    "arousal_max = newdf[\"arousal\"].max()\n",
    "bin_edges = np.linspace(arousal_min, arousal_max, num=30)  # Adjust num for bin count\n",
    "\n",
    "# Plot histograms with the same bin edges\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "sns.histplot(class_1[\"arousal\"], bins=bin_edges, color='blue', alpha=0.5, label=\"Class 1\")\n",
    "sns.histplot(class_2[\"arousal\"], bins=bin_edges, color='green', alpha=0.5, label=\"Class 2\")\n",
    "sns.histplot(class_3[\"arousal\"], bins=bin_edges, color='red', alpha=0.5, label=\"Class 3\")\n",
    "\n",
    "# Formatting\n",
    "plt.xlabel(\"Arousal Level\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.title(\"Histogram of Arousal Levels (Classes)\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newdf.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kept_newdf = newdf[[\"subject\",\"joy\", \"tenderness\", \"inspiration\", \"amusement\", \"anger\", \"disgust\", \"fear\", \"sadness\", \n",
    "                    \"arousal\", \"valence\", \"familiarity\", \"liking\", \"emotion_encoded\", \"stim_valence_encoded\"]]\n",
    "\n",
    "kept_newdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Select only the desired columns (excluding encoded columns)\n",
    "boxplot_vars = kept_newdf.drop(columns=[\"subject\",\"emotion_encoded\", \"stim_valence_encoded\"])\n",
    "\n",
    "# Plot boxplots\n",
    "plt.figure(figsize=(15, 8))\n",
    "sns.boxplot(data=boxplot_vars)\n",
    "plt.xticks(rotation=45)\n",
    "plt.title(\"Boxplot distribution of Emotions\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in kept_newdf.columns:\n",
    "    max = np.max(kept_newdf[i].values)\n",
    "    min = np.min(kept_newdf[i].values)\n",
    "    print(f\"{i} :\", max, min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"emotion_encoded\", set(newdf[\"emotion_encoded\"].values))\n",
    "print(\"stim_valence_encoded\", set(newdf[\"stim_valence_encoded\"].values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kept_newdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Split new_merged_df into 3 classes while preserving index\n",
    "three_classes = kept_newdf.shape[0] // 3\n",
    "class_1_m = kept_newdf.iloc[0:three_classes].copy()\n",
    "class_2_m = kept_newdf.iloc[three_classes:three_classes*2].copy()\n",
    "class_3_m = kept_newdf.iloc[three_classes*2:].copy()\n",
    "\n",
    "# Create y_label lists\n",
    "one_list = [1] * class_1_m.shape[0]\n",
    "two_list = [2] * class_2_m.shape[0]\n",
    "three_list = [3] * class_3_m.shape[0]\n",
    "\n",
    "# Assign labels\n",
    "class_1_m['y_label'] = one_list\n",
    "class_2_m['y_label'] = two_list\n",
    "class_3_m['y_label'] = three_list\n",
    "\n",
    "# Concatenate while preserving original index\n",
    "combined_df = pd.concat([class_1_m, class_2_m, class_3_m])\n",
    "\n",
    "# Ensure the index matches the original dataset\n",
    "assert all(combined_df.index == kept_newdf.index)\n",
    "\n",
    "# Print check\n",
    "print(combined_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df_sort =combined_df.sort_index()\n",
    "combined_df_sort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Count occurrences of each arousal label per subject\n",
    "subject_arousal_counts = combined_df_sort.groupby(['subject', 'y_label']).size().unstack()\n",
    "\n",
    "# Plot the distribution\n",
    "plt.figure(figsize=(15, 6))\n",
    "subject_arousal_counts.plot(kind='bar', stacked=True, colormap='viridis', figsize=(15, 6))\n",
    "\n",
    "# Labels and title\n",
    "plt.xlabel(\"Subject\")\n",
    "plt.ylabel(\"Count of Arousal Labels\")\n",
    "plt.title(\"Distribution of Arousal Labels per Subject in Arousal Set\")\n",
    "plt.legend(title=\"Arousal Label\")\n",
    "plt.xticks(rotation=90)\n",
    "plt.grid(axis='y', linestyle=\"--\", alpha=0.7)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get unique subjectsÂ¨\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split randomly at the row level\n",
    "train_df, test_df = train_test_split(combined_df, test_size=0.2, random_state=42, stratify=combined_df['y_label'])\n",
    "\n",
    "# Check class distribution\n",
    "print(train_df['y_label'].value_counts(normalize=True))  # Should be balanced\n",
    "print(test_df['y_label'].value_counts(normalize=True))   # Should be balanced\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subject_set = set(train_df[\"subject\"].values)\n",
    "print(len(list(subject_set)))\n",
    "print(subject_set)\n",
    "subjects = set(list(combined_df[\"subject\"]))\n",
    "print(subjects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "not_in_train = list(set(subjects).difference(subject_set))\n",
    "print(not_in_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict, Counter\n",
    "unique_vals = defaultdict(list)\n",
    "for i in list(subject_set):\n",
    "\n",
    "    number_1 = train_df[train_df[\"subject\"]==1]\n",
    "    Counting = Counter(number_1[\"y_label\"].values)\n",
    "    unique_vals[i] = Counting\n",
    "print(unique_vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Count occurrences of each arousal label per subject\n",
    "subject_arousal_counts = train_df.groupby(['subject', 'y_label']).size().unstack()\n",
    "\n",
    "# Plot the distribution\n",
    "plt.figure(figsize=(15, 6))\n",
    "subject_arousal_counts.plot(kind='bar', stacked=True, colormap='viridis', figsize=(15, 6))\n",
    "\n",
    "# Labels and title\n",
    "plt.xlabel(\"Subject\")\n",
    "plt.ylabel(\"Count of Arousal Labels\")\n",
    "plt.title(\"Distribution of Arousal Labels per Subject in Train Set\")\n",
    "plt.legend(title=\"Arousal Label\")\n",
    "plt.xticks(rotation=90)\n",
    "plt.grid(axis='y', linestyle=\"--\", alpha=0.7)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Count occurrences of each arousal label per subject\n",
    "subject_arousal_counts_test = test_df.groupby(['subject', 'y_label']).size().unstack()\n",
    "\n",
    "# Plot the distribution\n",
    "plt.figure(figsize=(15, 6))\n",
    "subject_arousal_counts_test.plot(kind='bar', stacked=True, colormap='viridis', figsize=(15, 6))\n",
    "\n",
    "# Labels and title\n",
    "plt.xlabel(\"Subject\")\n",
    "plt.ylabel(\"Count of Arousal Labels\")\n",
    "plt.title(\"Distribution of Arousal Labels per Subject in Test Set\")\n",
    "plt.legend(title=\"Arousal Label\")\n",
    "plt.xticks(rotation=90)\n",
    "plt.grid(axis='y', linestyle=\"--\", alpha=0.7)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_indices_df = pd.DataFrame(list(train_df.index), columns=['index'])\n",
    "test_indices_df = pd.DataFrame(list(test_df.index), columns=['index'])\n",
    "\n",
    "# Save to CSV\n",
    "#train_indices_df.to_csv(\"train_indices.csv\", index=False)\n",
    "#test_indices_df.to_csv(\"test_indices.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_indeces_reread = pd.read_csv(\"train_indices.csv\")\n",
    "test_indeces_reread = pd.read_csv(\"test_indices.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_indeces_reread"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(test_indeces_reread.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_index_list = []\n",
    "train_index_list = []\n",
    "for i_train in train_indeces_reread.values:\n",
    "\n",
    "    train_index_list.append(i_train[0])\n",
    "#print(test_index_list)\n",
    "\n",
    "for i_test in test_indeces_reread.values:\n",
    "    #print(i[0])\n",
    "    test_index_list.append(i_test[0])\n",
    "\n",
    "new_train_df = combined_df.iloc[train_index_list]\n",
    "new_test_df = combined_df.iloc[test_index_list]\n",
    "print(len(train_index_list))\n",
    "print(len(test_index_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the indices for each class\n",
    "def eeg_tensor_and_y_label(dataframe):\n",
    "    class_y_label_1 = dataframe[dataframe['y_label']==1] #= one_list\n",
    "    class_y_label_2 = dataframe[dataframe['y_label']==2] #= one_list\n",
    "    class_y_label_3 = dataframe[dataframe['y_label']==3] #= one_list\n",
    "    #class_2_m['y_label'] = two_list\n",
    "    #class_3_m['y_label'] = three_list\n",
    "\n",
    "\n",
    "\n",
    "    idx_class_1 = class_y_label_1.index.values\n",
    "    idx_class_2 = class_y_label_2.index.values\n",
    "    idx_class_3 = class_y_label_3.index.values\n",
    "\n",
    "    # Use these indices to extract the corresponding EEG tensors\n",
    "    eeg_class_1 = eeg[idx_class_1]\n",
    "    eeg_class_2 = eeg[idx_class_2]\n",
    "    eeg_class_3 = eeg[idx_class_3]\n",
    "    # Step 1: Create label arrays\n",
    "    labels_class_1 = [0] * eeg_class_1.shape[0]\n",
    "    labels_class_2 = [1] * eeg_class_2.shape[0]\n",
    "    labels_class_3 = [2] * eeg_class_3.shape[0]\n",
    "\n",
    "\n",
    "\n",
    "    # Assuming we have corresponding index lists for each dataset\n",
    "\n",
    "    # Step 2: Combine tensors and labels into list of tuples (tensor, label)\n",
    "    dataset_class_1 = [(eeg_class_1[i], labels_class_1[i]) for i in range(eeg_class_1.shape[0])]\n",
    "    dataset_class_2 = [(eeg_class_2[i], labels_class_2[i]) for i in range(eeg_class_2.shape[0])]\n",
    "    dataset_class_3 = [(eeg_class_3[i], labels_class_3[i]) for i in range(eeg_class_3.shape[0])]\n",
    "\n",
    "    dataset_mapping = {} \n",
    "    # Convert lists into a dictionary using their indexes\n",
    "    dataset_mapping.update(dict(zip(idx_class_1, dataset_class_1)))\n",
    "    dataset_mapping.update(dict(zip(idx_class_2, dataset_class_2)))\n",
    "    dataset_mapping.update(dict(zip(idx_class_3, dataset_class_3)))\n",
    "\n",
    "    # Retrieve samples in the exact order of dataframe.index\n",
    "    full_dataset = [dataset_mapping[idx] for idx in dataframe.index if idx in dataset_mapping]\n",
    "\n",
    "    return full_dataset\n",
    "\n",
    "    # Step 3: Check\n",
    "    print(len(dataset_class_1))\n",
    "    print(len(dataset_class_2))\n",
    "    print(len(dataset_class_3))\n",
    "    print(len(full_dataset))\n",
    "    return full_dataset\n",
    "\n",
    "\n",
    "train_tensor = eeg_tensor_and_y_label(new_train_df)\n",
    "test_tensor = eeg_tensor_and_y_label(new_test_df)\n",
    "\n",
    "torch.save(train_tensor, \"emotion_train_set.pt\")\n",
    "torch.save(test_tensor, \"emotion_test_set.pt\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(test_tensor)):\n",
    "    print(test_tensor[i][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import numpy as np\n",
    "# Define a method for training one epoch\n",
    "from torch.nn import Module\n",
    "from torch.optim.lr_scheduler import LRScheduler\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "\n",
    "def train_one_epoch(\n",
    "        dataloader: DataLoader, model: Module, loss_fn, optimizer,\n",
    "        scheduler: LRScheduler, epoch: int, device, print_batch_stats=True\n",
    "):\n",
    "    model.train()  # Set the model to training mode\n",
    "    train_loss, correct = 0, 0\n",
    "\n",
    "    progress_bar = tqdm(enumerate(dataloader), total=len(dataloader),\n",
    "                        disable=not print_batch_stats)\n",
    "\n",
    "    for batch_idx, (X, y) in progress_bar:\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        #print(X.shape)\n",
    "        optimizer.zero_grad()\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()  # update the model weights\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        correct += (pred.argmax(1) == y).sum().item()\n",
    "\n",
    "        #if print_batch_stats:\n",
    "        #    progress_bar.set_description(\n",
    "        #        f\"Epoch {epoch}/{n_epochs}, \"\n",
    "        #        f\"Batch {batch_idx + 1}/{len(dataloader)}, \"\n",
    "        #        f\"Loss: {loss.item():.6f}\"\n",
    "        #    )\n",
    "\n",
    "    # Update the learning rate\n",
    "    scheduler.step()\n",
    "\n",
    "    correct /= len(dataloader.dataset)\n",
    "    return train_loss / len(dataloader), correct\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import numpy as np\n",
    "\n",
    "@torch.no_grad()\n",
    "def test_model(dataloader: DataLoader, model: torch.nn.Module, loss_fn, print_batch_stats=True):\n",
    "    device = next(model.parameters()).device  # Get model device\n",
    "    size = len(dataloader.dataset)\n",
    "    n_batches = len(dataloader)\n",
    "    model.eval()  # Switch to evaluation mode\n",
    "    test_loss, correct = 0, 0\n",
    "\n",
    "    # Initialize dictionaries for per-class tracking\n",
    "    class_correct = defaultdict(int)\n",
    "    class_total = defaultdict(int)\n",
    "\n",
    "    # Lists to store true and predicted labels for confusion matrix\n",
    "    all_preds = []\n",
    "    all_targets = []\n",
    "\n",
    "    if print_batch_stats:\n",
    "        progress_bar = tqdm(enumerate(dataloader), total=len(dataloader))\n",
    "    else:\n",
    "        progress_bar = enumerate(dataloader)\n",
    "\n",
    "    for batch_idx, (X, y) in progress_bar:\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        pred = model(X)\n",
    "        batch_loss = loss_fn(pred, y).item()\n",
    "\n",
    "        test_loss += batch_loss\n",
    "        correct += (pred.argmax(1) == y).sum().item()\n",
    "\n",
    "        # Store predictions and true labels for confusion matrix\n",
    "        all_preds.append(pred.argmax(1).cpu())\n",
    "        all_targets.append(y.cpu())\n",
    "\n",
    "        # Compute per-class accuracy\n",
    "        preds_labels = pred.argmax(1)\n",
    "        for label, pred_label in zip(y, preds_labels):\n",
    "            class_total[label.item()] += 1\n",
    "            class_correct[label.item()] += (label == pred_label).item()\n",
    "\n",
    "        if print_batch_stats:\n",
    "            progress_bar.set_description(\n",
    "                f\"Batch {batch_idx + 1}/{len(dataloader)}, Loss: {batch_loss:.6f}\"\n",
    "            )\n",
    "\n",
    "    # Convert lists to tensors\n",
    "    all_preds = torch.cat(all_preds)\n",
    "    all_targets = torch.cat(all_targets)\n",
    "\n",
    "    # Compute per-class accuracy\n",
    "    class_accuracies = {\n",
    "        cls: (class_correct[cls] / class_total[cls]) * 100 if class_total[cls] > 0 else 0\n",
    "        for cls in class_total\n",
    "    }\n",
    "\n",
    "    # Compute overall accuracy\n",
    "    test_loss /= n_batches\n",
    "    overall_accuracy = (correct / size) * 100\n",
    "\n",
    "    # Print per-class accuracy\n",
    "    #print(\"\\nClass-wise Accuracy:\")\n",
    "    #for cls, acc in class_accuracies.items():\n",
    "    #    print(f\"  Class {cls}: {acc:.2f}%\")\n",
    "\n",
    "    #print(f\"Test Accuracy: {overall_accuracy:.1f}%, Test Loss: {test_loss:.6f}\\n\")\n",
    "\n",
    "    return test_loss, overall_accuracy, class_accuracies, all_preds, all_targets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!dir ..\\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cuda = torch.cuda.is_available()  # check if GPU is available, if True chooses to use it\n",
    "device = \"cuda\" if cuda else \"cpu\"\n",
    "if cuda:\n",
    "    torch.backends.cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#eeg.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collapsed_shallow_fbscp import ShallowFBCSPNet\n",
    "\n",
    "\n",
    "import collapsed_shallow_fbscp\n",
    "import importlib\n",
    "importlib.reload(collapsed_shallow_fbscp)\n",
    "# Now you can access the updated class\n",
    "from collapsed_shallow_fbscp import ShallowFBCSPNet\n",
    "\n",
    "model = ShallowFBCSPNet(\n",
    "    n_chans=eeg.shape[1],\n",
    "    n_subjs=9,\n",
    "    n_outputs=3,\n",
    "    n_times=eeg.shape[2],\n",
    "    #final_conv_length=\"auto\",\n",
    "    #edge_index = adjacency_matrix\n",
    ")\n",
    "\n",
    "# Display torchinfo table describing the model\n",
    "print(model)\n",
    "\n",
    "# Send model to GPU\n",
    "if cuda:\n",
    "    model.cuda()\n",
    "\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import wandb\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from torch.nn import CrossEntropyLoss\n",
    "import numpy as np\n",
    "\n",
    "# Initialize Weights & Biases\n",
    "wandb.init(project=\"Master Thesis\", name=\"ShallowFBCSPNet Accuracy\")\n",
    "\n",
    "# Define hyperparameters\n",
    "lr = 0.0001\n",
    "weight_decay = 1e-4\n",
    "batch_size = 124  # Start with 124\n",
    "n_epochs = 20\n",
    "\n",
    "# Log hyperparameters to wandb\n",
    "wandb.config.update({\n",
    "    \"learning_rate\": lr,\n",
    "    \"weight_decay\": weight_decay,\n",
    "    \"batch_size\": batch_size,\n",
    "    \"epochs\": n_epochs\n",
    "})\n",
    "\n",
    "# Define optimizer and scheduler\n",
    "optimizer = AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "scheduler = CosineAnnealingLR(optimizer, T_max=n_epochs - 1)\n",
    "\n",
    "# Define loss function\n",
    "loss_fn = CrossEntropyLoss()\n",
    "\n",
    "# Create DataLoaders\n",
    "train_loader = DataLoader(train_tensor, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_tensor, batch_size=batch_size)\n",
    "\n",
    "# Initialize lists to store all predictions & targets\n",
    "all_preds, all_targets = [], []\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(1, n_epochs + 1):\n",
    "    print(f\"Epoch {epoch}/{n_epochs}: \", end=\"\")\n",
    "\n",
    "    train_loss, train_accuracy = train_one_epoch(\n",
    "        train_loader, model, loss_fn, optimizer, scheduler, epoch, device\n",
    "    )\n",
    "\n",
    "    test_loss, test_accuracy, class_accuracies, batch_preds, batch_targets = test_model(test_loader, model, loss_fn)\n",
    "\n",
    "    # Store predictions & labels for confusion matrix\n",
    "    all_preds.extend(batch_preds)\n",
    "    all_targets.extend(batch_targets)\n",
    "\n",
    "    # Print class-wise accuracy\n",
    "    print(\"\\nClass-wise Accuracy:\")\n",
    "    for class_idx, acc in class_accuracies.items():\n",
    "        print(f\"  Class {class_idx}: {acc:.2f}%\")\n",
    "\n",
    "    # Log results to wandb\n",
    "    wandb.log({\n",
    "        \"epoch\": epoch,\n",
    "        \"train_loss\": train_loss,\n",
    "        \"train_accuracy\": train_accuracy * 100,\n",
    "        \"test_loss\": test_loss,\n",
    "        \"test_accuracy\": test_accuracy,\n",
    "        \"learning_rate\": scheduler.get_last_lr()[0],\n",
    "        **{f\"class_{class_idx}_accuracy\": acc for class_idx, acc in class_accuracies.items()}\n",
    "    })\n",
    "\n",
    "    print(\n",
    "        f\"Train Accuracy: {100 * train_accuracy:.2f}%, \"\n",
    "        f\"Average Train Loss: {train_loss:.6f}, \"\n",
    "        f\"Test Accuracy: {test_accuracy:.2f}%, \"\n",
    "        f\"Average Test Loss: {test_loss:.6f}\\n\"\n",
    "    )\n",
    "\n",
    "# Convert lists to NumPy arrays\n",
    "all_preds = np.array(all_preds)\n",
    "all_targets = np.array(all_targets)\n",
    "\n",
    "# Save predictions & true labels for later use (confusion matrix)\n",
    "wandb.log({\"all_preds\": all_preds.tolist(), \"all_targets\": all_targets.tolist()})\n",
    "\n",
    "wandb.finish()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import itertools\n",
    "import scipy.stats as stats  # Corrected import\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Step 1: Simulate a 4-class dataset\n",
    "np.random.seed(123)\n",
    "classes = [\"A\", \"B\", \"C\", \"D\"]\n",
    "observed = np.random.choice(classes, 100)  # True labels\n",
    "predicted = np.random.choice(classes, 100)  # Predicted labels\n",
    "\n",
    "# Create confusion matrix\n",
    "cm = confusion_matrix(observed, predicted, labels=classes)\n",
    "cm_df = pd.DataFrame(cm, index=classes, columns=classes)\n",
    "\n",
    "print(\"Confusion Matrix:\")\n",
    "print(cm_df)\n",
    "\n",
    "# Step 2: Classification Report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(observed, predicted, target_names=classes))\n",
    "\n",
    "# Step 3: Apply One-vs-All (OvA) approach\n",
    "def epi_tests_ova(cm, target_class, class_labels):\n",
    "    idx = class_labels.index(target_class)\n",
    "    \n",
    "    TP = cm[idx, idx]\n",
    "    FP = sum(cm[idx, :]) - TP\n",
    "    FN = sum(cm[:, idx]) - TP\n",
    "    TN = cm.sum() - (TP + FP + FN)\n",
    "    \n",
    "    contingency_table = np.array([[TP, FP], [FN, TN]])\n",
    "    chi2, p, dof, expected = stats.chi2_contingency(contingency_table)  # Fixed function call\n",
    "    \n",
    "    return {\"Class\": target_class, \"TP\": TP, \"FP\": FP, \"FN\": FN, \"TN\": TN, \"Chi2\": chi2, \"p-value\": p}\n",
    "\n",
    "# Apply epi_tests_ova for each class\n",
    "results = [epi_tests_ova(cm, cls, classes) for cls in classes]\n",
    "\n",
    "# Step 4: Convert Results into a Summary Table\n",
    "summary_table = pd.DataFrame(results)\n",
    "\n",
    "# Round numeric values\n",
    "summary_table.iloc[:, 1:] = summary_table.iloc[:, 1:].apply(lambda x: np.round(x, 2))\n",
    "\n",
    "print(\"\\nSummary Table of Metrics:\")\n",
    "print(summary_table)\n",
    "\n",
    "# Step 5: Export Results to CSV\n",
    "summary_table.to_csv(\"multi_class_metrics.csv\", index=False)\n",
    "\n",
    "# Step 6: Plot Confusion Matrix\n",
    "plt.figure(figsize=(6, 5))\n",
    "plt.imshow(cm, cmap=\"Blues\", interpolation=\"nearest\")\n",
    "plt.colorbar()\n",
    "plt.xticks(ticks=np.arange(len(classes)), labels=classes)\n",
    "plt.yticks(ticks=np.arange(len(classes)), labels=classes)\n",
    "\n",
    "# Add text annotations\n",
    "for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "    plt.text(j, i, cm[i, j], horizontalalignment=\"center\", color=\"black\")\n",
    "\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Observed\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "eeg_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
