{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from braindecode.models import EEGConformer\n",
    "import pandas as pd\n",
    "from collections import OrderedDict\n",
    "import math\n",
    "import importlib\n",
    "import Attention_temporal_model\n",
    "importlib.reload(Attention_temporal_model)\n",
    "from Attention_temporal_model import ShallowAttentionNet\n",
    "from shallow_laurits import ShallowFBCSPNet\n",
    "from torchinfo import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "cannot assign module before Module.__init__() call",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[25], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m n_classes \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m4\u001b[39m\n\u001b[0;32m      3\u001b[0m input_window_samples \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1125\u001b[39m\n\u001b[1;32m----> 4\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mShallowAttentionNet\u001b[49m\u001b[43m(\u001b[49m\u001b[43min_chans\u001b[49m\u001b[43m,\u001b[49m\u001b[43mn_classes\u001b[49m\u001b[43m,\u001b[49m\u001b[43minput_window_samples\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ahmm9\\Documents\\Github\\Thesis_Rep_similarity\\Pipeline\\python_models\\Attention_FIRST_model.py:48\u001b[0m, in \u001b[0;36mShallowAttentionNet.__init__\u001b[1;34m(self, n_chans, n_outputs, n_times, dropout, num_kernels, kernel_size, pool_size)\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_outputs \u001b[38;5;241m=\u001b[39m n_outputs\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_times \u001b[38;5;241m=\u001b[39m n_times\n\u001b[1;32m---> 48\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtemporal_attention\u001b[49m \u001b[38;5;241m=\u001b[39m TemporalAttention(in_channels\u001b[38;5;241m=\u001b[39mn_chans)\n\u001b[0;32m     49\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspatial \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mConv2d(num_kernels, num_kernels, (n_chans, \u001b[38;5;241m1\u001b[39m))\n\u001b[0;32m     50\u001b[0m \u001b[38;5;66;03m# **Spatial Attention Module (Replaces Temporal & Spatial Conv)**\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1757\u001b[0m, in \u001b[0;36mModule.__setattr__\u001b[1;34m(self, name, value)\u001b[0m\n\u001b[0;32m   1755\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(value, Module):\n\u001b[0;32m   1756\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m modules \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1757\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\n\u001b[0;32m   1758\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcannot assign module before Module.__init__() call\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   1759\u001b[0m     remove_from(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_parameters, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_buffers, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_non_persistent_buffers_set)\n\u001b[0;32m   1760\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m _global_module_registration_hooks\u001b[38;5;241m.\u001b[39mvalues():\n",
      "\u001b[1;31mAttributeError\u001b[0m: cannot assign module before Module.__init__() call"
     ]
    }
   ],
   "source": [
    "in_chans = 22\n",
    "n_classes = 4\n",
    "input_window_samples = 1125\n",
    "model = ShallowAttentionNet(in_chans,n_classes,input_window_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ShallowAttentionNet(\n",
      "  (spatial_att): SpatialAttention(\n",
      "    (query): Linear(in_features=22, out_features=22, bias=False)\n",
      "    (key): Linear(in_features=22, out_features=22, bias=False)\n",
      "    (value): Linear(in_features=22, out_features=22, bias=False)\n",
      "    (out_linear): Linear(in_features=22, out_features=22, bias=False)\n",
      "  )\n",
      "  (temporal): Conv2d(1, 10, kernel_size=(1, 25), stride=(1, 1))\n",
      "  (batch_norm): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (pool): AvgPool2d(kernel_size=(1, 100), stride=(1, 100), padding=0)\n",
      "  (dropout): Dropout(p=0.7, inplace=False)\n",
      "  (fc): LazyLinear(in_features=0, out_features=4, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "after unsqueeze: torch.Size([2, 1, 22, 1125])\n",
      "torch.Size([2, 22, 1125])\n",
      "torch.Size([2, 1125, 22])\n",
      "torch.Size([2, 1125, 22])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "============================================================================================================================================\n",
       "Layer (type:depth-idx)                   Input Shape               Output Shape              Param #                   Kernel Shape\n",
       "============================================================================================================================================\n",
       "ShallowAttentionNet                      [2, 22, 1125]             [2, 4]                    --                        --\n",
       "├─SpatialAttention: 1-1                  [2, 1, 22, 1125]          [2, 1, 22, 1125]          1                         --\n",
       "│    └─Linear: 2-1                       [2, 1125, 22]             [2, 1125, 22]             484                       --\n",
       "│    └─Linear: 2-2                       [2, 1125, 22]             [2, 1125, 22]             484                       --\n",
       "│    └─Linear: 2-3                       [2, 1125, 22]             [2, 1125, 22]             484                       --\n",
       "│    └─Linear: 2-4                       [2, 1125, 22]             [2, 1125, 22]             484                       --\n",
       "├─Conv2d: 1-2                            [2, 1, 22, 1125]          [2, 10, 22, 1101]         260                       [1, 25]\n",
       "├─BatchNorm2d: 1-3                       [2, 10, 22, 1101]         [2, 10, 22, 1101]         20                        --\n",
       "├─AvgPool2d: 1-4                         [2, 10, 22, 1101]         [2, 10, 22, 11]           --                        [1, 100]\n",
       "├─Dropout: 1-5                           [2, 2420]                 [2, 2420]                 --                        --\n",
       "├─Linear: 1-6                            [2, 2420]                 [2, 4]                    9,684                     --\n",
       "============================================================================================================================================\n",
       "Total params: 11,901\n",
       "Trainable params: 11,901\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (Units.MEGABYTES): 12.62\n",
       "============================================================================================================================================\n",
       "Input size (MB): 0.20\n",
       "Forward/backward pass size (MB): 9.34\n",
       "Params size (MB): 0.05\n",
       "Estimated Total Size (MB): 9.58\n",
       "============================================================================================================================================"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary(model,input_size=(2, 22, 1125),col_names=[\"input_size\", \"output_size\", \"num_params\", \"kernel_size\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
