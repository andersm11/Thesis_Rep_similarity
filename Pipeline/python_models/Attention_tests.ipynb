{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from braindecode.models import EEGConformer\n",
    "import pandas as pd\n",
    "from collections import OrderedDict\n",
    "import math\n",
    "import importlib\n",
    "import Spatial_attention_model\n",
    "importlib.reload(Spatial_attention_model)\n",
    "from Spatial_attention_model import ShallowAttentionNet\n",
    "from shallow_laurits import ShallowFBCSPNet\n",
    "from torchinfo import summary\n",
    "from weight_init import init_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ShallowAttentionNet(\n",
       "  (temporal): Conv2d(1, 10, kernel_size=(1, 25), stride=(1, 1))\n",
       "  (spatial_att): SpatialAttention(\n",
       "    (conv): Conv2d(10, 10, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "  )\n",
       "  (batch_norm): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (pool): AvgPool2d(kernel_size=(1, 20), stride=(1, 20), padding=0)\n",
       "  (dropout): Dropout(p=0.7, inplace=False)\n",
       "  (fc): Linear(in_features=12100, out_features=4, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "in_chans = 22\n",
    "n_classes = 4\n",
    "input_window_samples = 1125\n",
    "model = ShallowAttentionNet(in_chans,n_classes,input_window_samples)\n",
    "model.apply(init_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ShallowAttentionNet(\n",
      "  (temporal): Conv2d(1, 10, kernel_size=(1, 25), stride=(1, 1))\n",
      "  (spatial_att): SpatialAttention(\n",
      "    (conv): Conv2d(10, 10, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "  )\n",
      "  (batch_norm): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (pool): AvgPool2d(kernel_size=(1, 20), stride=(1, 20), padding=0)\n",
      "  (dropout): Dropout(p=0.7, inplace=False)\n",
      "  (fc): Linear(in_features=12100, out_features=4, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([20, 22, 1101])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "============================================================================================================================================\n",
       "Layer (type:depth-idx)                   Input Shape               Output Shape              Param #                   Kernel Shape\n",
       "============================================================================================================================================\n",
       "ShallowAttentionNet                      [2, 22, 1125]             [2, 4]                    --                        --\n",
       "├─Conv2d: 1-1                            [2, 1, 22, 1125]          [2, 10, 22, 1101]         260                       [1, 25]\n",
       "├─SpatialAttention: 1-2                  [2, 10, 22, 1101]         [2, 10, 22, 1101]         --                        --\n",
       "│    └─Conv2d: 2-1                       [2, 10, 22, 1101]         [2, 10, 22, 1101]         100                       [1, 1]\n",
       "├─BatchNorm2d: 1-3                       [2, 10, 22, 1101]         [2, 10, 22, 1101]         20                        --\n",
       "├─AvgPool2d: 1-4                         [2, 10, 22, 1101]         [2, 10, 22, 55]           --                        [1, 20]\n",
       "├─Dropout: 1-5                           [2, 12100]                [2, 12100]                --                        --\n",
       "├─Linear: 1-6                            [2, 12100]                [2, 4]                    48,404                    --\n",
       "============================================================================================================================================\n",
       "Total params: 48,784\n",
       "Trainable params: 48,784\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (Units.MEGABYTES): 17.54\n",
       "============================================================================================================================================\n",
       "Input size (MB): 0.20\n",
       "Forward/backward pass size (MB): 11.63\n",
       "Params size (MB): 0.20\n",
       "Estimated Total Size (MB): 12.02\n",
       "============================================================================================================================================"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary(model,input_size=(2, 22, 1125),col_names=[\"input_size\", \"output_size\", \"num_params\", \"kernel_size\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
